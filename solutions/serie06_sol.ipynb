{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a0307a-ef21-4b03-8904-ce4ab45e8a28",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cc735b58fcec6c99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Numerical Analysis - Fall semester 2024\n",
    "\n",
    "# Serie 06 - Least squares fitting and numerical differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c9b67-8e23-4d7f-9108-7c7db302e016",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-84f0b69af87d0e2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Importing NumPy and matplotlib, our loyal companions. This time, we will also generate some data for you to use in one of the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25045b-6082-4329-9851-75e55d07bfc9",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-49dfbb096c10637f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(0); np.savetxt(\"data.txt\", np.vstack((np.linspace(0, 10, 20), np.sin(np.linspace(0, 10, 20)) + np.linspace(0, 10, 20) + 0.1 * np.random.randn(20))).T); np.savetxt(\"big_data.txt\", np.vstack((np.linspace(0, 10, 20000), np.sin(np.linspace(0, 10, 20000)) + np.linspace(0, 10, 20000) + 0.1 * np.random.randn(20000))).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f01c43-6287-4071-b270-a5854c2ef723",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dcb85adf51dc30cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "### Least squares approximation of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad11262-5d5f-4818-9329-20af5068349e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5924054146106def",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us consider the data $(x_i,y_i)$, $i=1,\\ldots,20$ contained in the file `data.txt` which has been generated for you in the same directory as this notebook is stored in. The data corresponds to the evaluation of the function \n",
    "\n",
    "$$\n",
    "  f(x) = \\sin(x) + x, \\quad x\\in I=[0, 10],\n",
    "$$\n",
    "\n",
    "on $n+1=20$ equally distributed nodes (therefore $x_1=0$ and $x_{20}=10$).\n",
    "The evaluations are perturbed by Gaussian noise as follows:\n",
    "\n",
    "$$\n",
    "y_i=f(x_i) + \\varepsilon_i, \\quad i=1,\\ldots,n+1.\n",
    "$$\n",
    "\n",
    "Let us suppose that the terms $\\varepsilon_i$ are stochastically independent, with zero mean and standard deviation $\\sigma=0.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cb9a98-51b1-4658-842b-da0c49472d8e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cbc6cd8f874059f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 1:** Load the data using the function `np.loadtxt('[FILENAME HERE].txt')` and visualize it in a plot. Also add a visualization of the function $f$ in your plot.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7be73d-4642-415b-9d8f-680c1beb6e6d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c45265f255105923",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    ### BEGIN SOLUTION\n",
    "    return np.sin(x) + x\n",
    "    ### END SOLUTION\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "data = np.loadtxt(\"data.txt\")\n",
    "x = data[:, 0]\n",
    "y = data[:, 1]\n",
    "x_lin = np.linspace(0, 10, 100)\n",
    "plt.scatter(x, y, label=r\"data $(x_i, y_i)$\")\n",
    "plt.plot(x_lin, f(x_lin), color=\"black\", label=r\"function $f(x)$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07edc9a9-b290-4bf5-a524-405b4a876064",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-63e41a2916c6a825",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 2:** For $m = 4, 7, 15$, compute and plot the least-squares polynomial $p_{m}^{LS}$ of degree $m$ of the data. Do this in a separate figure for each $m$ where you also plot the function and the data. What do you observe?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c08346-dd45-4305-b237-56f47fecc5e4",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb6ad7a07390cd65",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "m_list = [4, 7, 15]\n",
    "for m in m_list:\n",
    "    coeff = np.polyfit(x, y, m)\n",
    "    y_leastsquares = np.polyval(coeff, x_lin)\n",
    "    plt.scatter(x, y, label=r\"data $(x_i, y_i)$\")\n",
    "    plt.plot(x_lin, f(x_lin), color=\"black\", label=r\"function $f(x)$\")\n",
    "    plt.plot(x_lin, y_leastsquares, color=\"red\", label=f\"least squares polynomial ($m = {m}$)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# We notice that when the degree increases, the precision of the interpolation\n",
    "# decreases. In fact, the least squares polynomial of degree m=7 gives in this\n",
    "# case a better approximation than the one of degree m=15.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24eb30e-e396-42c0-9415-fba22b5a2538",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0aa7223141fd86a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 3:** Evaluate the approximation error\n",
    "\n",
    "$$\n",
    "E(p_m^{LS},f)=\\max_{x \\in [0,10]} \\vert f(x) - p_m^{LS}(x) \\vert\n",
    "$$\n",
    "\n",
    "for $m=1,\\ldots,15$. (Do this on a very fine grid.) Then plot the error in terms of $m$ in semi-logarithmic scale. What behavior do you observe for high values of $m$? For which value of $m$ is the approximation error the smallest?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d24a7-adfc-44b4-ac49-167dd579d029",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e4ec6d9523788126",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "errors = []\n",
    "m_list = np.arange(1, 16)\n",
    "for m in m_list:\n",
    "    coeff = np.polyfit(x, y, m)\n",
    "    x_lin_fine = np.linspace(0, 10, 10000)\n",
    "    y_leastsquares = np.polyval(coeff, x_lin_fine)\n",
    "    y_true = f(x_lin_fine)\n",
    "    errors.append(np.max(np.abs(y_true - y_leastsquares)))\n",
    "\n",
    "plt.semilogy(m_list, errors)\n",
    "plt.xlabel(r\"$m$\")\n",
    "plt.ylabel(r\"approximation error\")\n",
    "plt.grid(True, which=\"major\",ls=\"-\")\n",
    "plt.grid(True, which=\"minor\",ls=\"--\")\n",
    "plt.ylim(0.09, 1.5)\n",
    "plt.show()\n",
    "\n",
    "# We notice that the polynomial of degree m=7 gives a better approximation than\n",
    "# m=4. However, the polynomial of degree m=15 gives a bad approximation at the\n",
    "# extremities of the interval. This is due to the fact that the least squares\n",
    "# approximation becomes more and more unstable (for equally distributed nodes)\n",
    "# when m approaches n.\n",
    "\n",
    "# We can notice that the approximation error decreases up to the value m=6, \n",
    "# but afterwards the error increases as the number of data points (n=19) is\n",
    "# not big enough.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa968a-95ed-4689-9ad6-f4547ea06f1f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3d8db571897ea9cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 4:** For $m=4$, estimate the variance of the noise using the formula \n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2=\\frac{1}{n-m}\\sum_{i=1}^{n+1}\\left(y_i-p_m^{LS}(x_i)\\right)^2\n",
    "$$\n",
    "\n",
    "whose description you can find in the lecture notes on page 53. Check that in the previous exercise, the smallest value of $E(p_m^{LS},f)$ for any $m$ is of the order of $\\hat{\\sigma}\\sqrt{\\frac{m+1}{n+1}}$. Also,\n",
    "compare the smallest error with $\\sigma\\sqrt{\\frac{m+1}{n+1}}$. Remember that $n+1 = 20$ was the number of data points and $\\sigma = 0.1$ the standard deviation of the noise we added to the data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6741b2-b088-47e1-8002-784c70304d68",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3ef21d0a9dc1d1de",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "n = len(x) - 1\n",
    "m = 4\n",
    "coeff = np.polyfit(x, y, m)\n",
    "y_leastsquares = np.polyval(coeff, x)\n",
    "\n",
    "sigma2_est = np.sum((y - y_leastsquares)**2) / (n - m)\n",
    "print(\"estimated variance of noise: σ² = {:.4f}\".format(sigma2_est))\n",
    "\n",
    "min_error_est = np.sqrt(sigma2_est) * np.sqrt((m + 1) / (n + 1))\n",
    "print(\"estimated smallest error: σ √(m+1)/(n+1) = {:.4f}\".format(min_error_est))\n",
    "\n",
    "sigma = 0.1\n",
    "min_error_eff = sigma * np.sqrt((m + 1) / (n + 1))\n",
    "print(\"theoretical smallest error: σ √(m+1)/(n+1) = {:.4f}\".format(min_error_eff))\n",
    "\n",
    "# In the presence measurement errors, it is usually not possible to achieve\n",
    "# zero error with a least squares polynomial. If the errors are iid random\n",
    "# variables with expected value zero and variance σ² > 0, one can prove that\n",
    "# the error E(p_m, f) is of the order of σ √(m+1)/(n+1). The computed estimate\n",
    "# for this error quite accurately reflects the actually observed smallest error\n",
    "# in the above plot.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e41ce0-0a01-4459-849c-5bf2f1731111",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-156bcd0b077ab985",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 5:** Repeat all of the above exercises with the data contained in the file `big_data.txt`, which includes $n+1=20000$ data points following the same model as the data in `data.txt`. Compare for which $m$ the smallest approximation error is obtained for the two data sets and how big it is respectively.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927f8a8-92f6-453f-9bb9-934022a097e6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-12f0869c68e6e15e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "def f(x):\n",
    "    return np.sin(x) + x\n",
    "\n",
    "# Plot data\n",
    "data = np.loadtxt(\"big_data.txt\")\n",
    "x = data[:, 0]\n",
    "y = data[:, 1]\n",
    "x_lin = np.linspace(0, 10, 100)\n",
    "plt.scatter(x, y, label=r\"data $(x_i, y_i)$\")\n",
    "plt.plot(x_lin, f(x_lin), color=\"black\", label=r\"function $f(x)$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot least squares approximants\n",
    "m_list = [4, 7, 15]\n",
    "for m in m_list:\n",
    "    coeff = np.polyfit(x, y, m)\n",
    "    y_leastsquares = np.polyval(coeff, x_lin)\n",
    "    plt.scatter(x, y, label=r\"data $(x_i, y_i)$\")\n",
    "    plt.plot(x_lin, f(x_lin), color=\"black\", label=r\"function $f(x)$\")\n",
    "    plt.plot(x_lin, y_leastsquares, color=\"red\", label=f\"least squares polynomial ($m = {m}$)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot errors\n",
    "errors = []\n",
    "m_list = np.arange(1, 16)\n",
    "for m in m_list:\n",
    "    coeff = np.polyfit(x, y, m)\n",
    "    x_lin_fine = np.linspace(0, 10, 1000)\n",
    "    y_leastsquares = np.polyval(coeff, x_lin_fine)\n",
    "    y_true = f(x_lin_fine)\n",
    "    errors.append(np.max(np.abs(y_true - y_leastsquares)))\n",
    "\n",
    "plt.semilogy(m_list, errors)\n",
    "plt.xlabel(r\"$m$\")\n",
    "plt.ylabel(r\"interpolation error\")\n",
    "plt.grid(True, which=\"major\",ls=\"-\")\n",
    "plt.grid(True, which=\"minor\",ls=\"--\")\n",
    "plt.show()\n",
    "\n",
    "# Compute variance/noise estimators\n",
    "n = len(x) - 1\n",
    "m = 4\n",
    "coeff = np.polyfit(x, y, m)\n",
    "y_leastsquares = np.polyval(coeff, x)\n",
    "\n",
    "sigma2_est = np.sum((y - y_leastsquares)**2) / (n - m)\n",
    "print(\"estimated variance of noise: σ² = {:.4f}\".format(sigma2_est))\n",
    "\n",
    "min_error_est = np.sqrt(sigma2_est) * np.sqrt((m + 1) / (n + 1))\n",
    "print(\"estimated smallest error: σ √(m+1)/(n+1) = {:.4f}\".format(min_error_est))\n",
    "\n",
    "sigma = 0.1\n",
    "min_error_eff = sigma * np.sqrt((m + 1) / (n + 1))\n",
    "print(\"theoretical smallest error: σ √(m+1)/(n+1) = {:.4f}\".format(min_error_eff))\n",
    "\n",
    "# With n+1=20000 nodes, the least squares polynomials of degree m ≤ 15 are\n",
    "# accurate since the number of data is very high. Therefore, the approximation\n",
    "# error decreases almost up to the value m = 11.\n",
    "\n",
    "# The minimal approximation error is obtained for m = 11 and is around 0.006\n",
    "# which is approximately the same as the estimate σ √(m+1)/(n+1).\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f114c0ce-8ecf-43d2-9622-22097666d9d5",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6d7f999d0bbb6be0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "### Mysterious finite differences formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432803a1-a37c-4c0a-a39d-df80d0356b4e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-743b7a3e7bc0a719",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 6 (Theoretical):** Consider the general finite differences formula\n",
    "\n",
    "$$\n",
    "D_hf({x})=\\frac{\\frac{1}{3} f({x}+h)+\\beta\\, f({x})- (\\frac{1}{2}+\\beta)\\,f(x-h)+\n",
    "\\frac{1}{6}f(x-2h)}{h}\n",
    "$$\n",
    "\n",
    "to approximate $f'(x)$ with $\\beta\\, \\in \\mathbb{R}$ independent of $h$. If we suppose that the function $f$ is regular enough, which of the following statements is true? [Only one answer is possible]\n",
    "\n",
    "1. If $\\beta = \\frac{1}{2}$, the formula $D_h f$ is of second order but not of third order.\n",
    "\n",
    "2. If $\\beta \\neq \\frac{1}{2}$, the formula does not approximate $f'(x)$ when $h\\rightarrow 0$.\n",
    "\n",
    "3. If $\\beta = -\\frac{5}{6}$, the formula $D_h f$ is of first order.\n",
    "\n",
    "4. The formula $D_h f$ converges to $f'(x)$ for all $\\beta \\in \\mathbb{R}$ when $h\\rightarrow 0$.\n",
    "\n",
    "5. If $\\beta = -\\frac{1}{6}$, the formula $D_h f$ is of first order.\n",
    "\n",
    "*Hint:* Insert the Taylor series expansions of $f(x + h)$, $f(x - h)$, and $f(x - 2h)$ into the formula and group all terms of the same derivative order ($f(x),f'(x),f''(x), \\dots$) together to see how it behaves depending on $\\beta$ and $h$.\n",
    "</div>\n",
    "\n",
    "=== BEGIN MARK SCHEME ===\n",
    "\n",
    "The answer 2 is correct. Expanding $f(x + h)$, $f(x - h)$, and $f(x - 2h)$ in Taylor series of first order around $x$, we get\n",
    "\n",
    "\\begin{align*}\n",
    "D_h f(x) = \\frac{1}{h} \n",
    "\t\\Bigg[&\\frac{1}{3} \\bigg(f(x) + hf'(x) + \\mathcal{O}(h^2) \\bigg) \\\\\n",
    "           &+ \\beta f(x) \\\\\n",
    "           &-\\left( \\frac{1}{2} + \\beta \\right) \\bigg( f(x) - h f'(x) + \\mathcal{O}(h^2)  \\bigg) \\\\\n",
    "       &\\frac{1}{6} \\bigg( f(x) - 2 h f'(x) +  \\mathcal{O}(h^2)  \\bigg)\n",
    "\t\\Bigg].\n",
    "\\end{align*}\n",
    "\n",
    "Regrouping the terms yields \n",
    "\n",
    "\\begin{align*}\n",
    "D_h f(x) &= f(x) \\frac{1}{h} \\underbrace{\\bigg( \\frac{1}{3} + \\beta - \\frac{1}{2} - \\beta + \\frac{1}{6} \\bigg)}_{=0} + f'(x) \\underbrace{\\bigg( \\frac{1}{3} + \\frac{1}{2} + \\beta - \\frac{1}{3} \\bigg)}_{= \\frac{1}{2} + \\beta} + \\mathcal{O}(h) \\\\\n",
    "&= f'(x)\\bigg(\\frac{1}{2} + \\beta\\bigg) + \\mathcal{O}(h).\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, unless $\\beta = \\frac{1}{2}$, $D_h f(x)$ will not approximate $f'(x)$ when $h \\to 0$.\n",
    "\n",
    "=== END MARK SCHEME ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f885584-7c3c-4987-98ed-6e3ca8d5fdfd",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3141cab342a74bd8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "### Forward and central finite differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db6b58-fa15-41c9-8867-4cc70c9a86c6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-36f727029cd3481f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 7:** Implement the functions `delta_h_forward` and `delta_h_central` which, given a function $f$, a point $\\overline{x}$, and a value $h$, compute the forward finite differences approximation \n",
    "\\begin{equation}\n",
    "\\delta_h^{+}f(\\overline{x}) = \\frac{f(\\overline{x} + h) - f(\\overline{x})}{h}\n",
    "\\end{equation}\n",
    "and the central finite difference approximation \n",
    "\\begin{equation}\n",
    "\\delta_h^{c}f(\\overline{x}) = \\frac{f(\\overline{x} + h) - f(\\overline{x} - h)}{2h}\n",
    "\\end{equation}\n",
    "respectively.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db09e5-d835-4c12-abd2-9c0da7cae88a",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3bae7b0b57da42c1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def delta_h_forward(f, x_bar, h):\n",
    "    ### BEGIN SOLUTION\n",
    "    return (f(x_bar + h) - f(x_bar)) / h\n",
    "    ### END SOLUTION\n",
    "\n",
    "def delta_h_central(f, x_bar, h):\n",
    "    ### BEGIN SOLUTION\n",
    "    return (f(x_bar + h) - f(x_bar - h)) / (2 * h)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24617191-0075-4057-8516-d1411d393f15",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9d11fe8c9f9f17c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us run a few simple checks on your implementation, to see if there are no issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394e023-0442-494a-970e-556f5195d1d3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3f4849577575e280",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.isclose(D := delta_h_forward(lambda x: x, 0, 0.1), 1), f\"'delta_h_forward(f, 0, 0.1)' for 'f(x) = x' should return 'f'(0) ≈ 1', but got {D}\";assert np.isclose(D := delta_h_central(lambda x: x, 0, 0.1), 1), f\"'delta_h_central(f, 0, 0.1)' for 'f(x) = x' should return 'f'(0) ≈ 1', but got {D}\";assert np.isclose(D := delta_h_forward(lambda x: x**2, 0.5, 1e-10), 1), f\"'delta_h_forward(f, 0, 0.1)' for 'f(x) = x^2' should return 'f'(1/2) ≈ 1', but got {D}\";assert np.isclose(D := delta_h_central(lambda x: x**2, 0.5, 1e-10), 1), f\"'delta_h_central(f, 0, 0.1)' for 'f(x) = x^2' should return 'f'(1/2) ≈ 1', but got {D}\";print(\"Nice! Your function worked well on the simple examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344b2477-a604-4b8b-a335-49885b4437b5",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d3b2430fda49eada",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We now consider the function\n",
    "\n",
    "\\begin{equation}\n",
    "f(x)=x\\log(x)-\\sin^2(x)\n",
    "\\end{equation}\n",
    "\n",
    "for $x>0$ for which we want to approximate the first derivative in\n",
    "$\\bar{x}=1.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dc8e60-e4f2-4274-af7d-bc1974859d6d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a8bcca3c6359c15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Exercise 8:** Approximate the derivative of $f$ using the two methods you have implemented in the previous exercises for $h = 0.1 \\cdot (\\frac{1}{2})^{i}$ for $i=0,1,2,3,4,5$, and compute for each $h$ the error $\\varepsilon_h=\\vert f^{'}(\\bar{x})-\\delta_h  (\\bar{x})\\vert$, where $\\delta_h$ is either the forward finite differences or the central finite differences operator. Plot the errors $\\varepsilon_h$ in terms of $h$ on a graph in logarithmic scale. Comment on the obtained results.\n",
    "\n",
    "*Hint:* Use `plt.loglog` and compare the graphs of $\\varepsilon_h$ with the graphs defined by the curves $(h,h)$ and $(h,h^2)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9ae130-d571-42ed-a051-48fd44e41070",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4025f88f67eb6a6a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    ### BEGIN SOLUTION\n",
    "    return x * np.log(x) - np.sin(x) ** 2\n",
    "    ### END SOLUTION\n",
    "\n",
    "def df(x):\n",
    "    ### BEGIN SOLUTION\n",
    "    return 1 + np.log(x) - 2 * np.sin(x) * np.cos(x)\n",
    "    ### END SOLUTION\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "x_bar = 1\n",
    "errors_forward = []\n",
    "errors_central = []\n",
    "\n",
    "h_list = 0.1 * 0.5 ** np.arange(6)\n",
    "for h in h_list:\n",
    "    errors_forward.append(abs(df(x_bar) - delta_h_forward(f, x_bar, h)))\n",
    "    errors_central.append(abs(df(x_bar) - delta_h_central(f, x_bar, h)))\n",
    "\n",
    "plt.loglog(h_list, errors_forward, label=r\"$\\varepsilon_h$ forward\")\n",
    "plt.loglog(h_list, errors_central, label=r\"$\\varepsilon_h$ central\")\n",
    "plt.loglog(h_list, h_list, linestyle=\"--\", c=\"tab:blue\", label=r\"$(h, h)$\")\n",
    "plt.loglog(h_list, h_list ** 2, linestyle=\"--\", c=\"tab:orange\", label=r\"$(h, h^2)$\")\n",
    "plt.xlabel(r\"$h$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# We notice that for the forward finite differences method, the error decreases\n",
    "# with a slope (h, h) in logarithmic scale, showing that the method is of first\n",
    "# order. For the central finite differences method, the error decreases with\n",
    "# slope (h, h^2), which means it is of second order.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60003d1a-3db4-4f7c-9cd3-f6d646e96d90",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-73274d942c17aede",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 9:** Repeat the previous exercise for $i = 0, 1,\\ldots, 30$. Comment the results.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a974f3f0-0f76-4bf2-b43e-93392e7fbecf",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-149a97ac6376015a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "x_bar = 1\n",
    "errors_forward = []\n",
    "errors_central = []\n",
    "\n",
    "h_list = 0.1 * 0.5 ** np.arange(31)\n",
    "for h in h_list:\n",
    "    errors_forward.append(abs(df(x_bar) - delta_h_forward(f, x_bar, h)))\n",
    "    errors_central.append(abs(df(x_bar) - delta_h_central(f, x_bar, h)))\n",
    "\n",
    "plt.loglog(h_list, errors_forward, label=r\"$\\varepsilon_h$ forward\")\n",
    "plt.loglog(h_list, errors_central, label=r\"$\\varepsilon_h$ central\")\n",
    "plt.loglog(h_list, h_list, linestyle=\"--\", c=\"tab:blue\", label=r\"$(h, h)$\")\n",
    "plt.loglog(h_list, h_list ** 2, linestyle=\"--\", c=\"tab:orange\", label=r\"$(h, h^2)$\")\n",
    "plt.xlabel(r\"$h$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# We notice that for the forward finite differences method, the errors decreases\n",
    "# initially with slope (h, h) in logarithmic scale and then, for h < √1e-16 = 1e-8,\n",
    "# it increases with slope (h, 1/h). This is due to round-off errors. Similarly,\n",
    "# for the central finite differences method, the error decreases initially with\n",
    "# slope (h, h^2) and then, for h < ∛1e-16 ≈ 1e-5, it increases with slope (h, 1/h)\n",
    "# because of the round-off errors.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdff9f2-73c0-4844-a201-e75fd354a0ae",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e6eabae26c04ed09",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "## The end\n",
    "\n",
    "Amazing! You have reached the end of the sixth exercise notebook. We wish you a relaxing study break!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
