{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278078d9-126a-4344-b036-504006b85980",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7210cc7c6d0b974e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Numerical Analysis - Fall semester 2024\n",
    "# Serie 04 - Variants of Newton's method and Polynomial Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa6a5cf-cf1f-4283-8afe-2c5acf1d4a2c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0a827a51c9b09174",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As usual, we will import some useful packages for later reference. You will have to run this cell every time you restart your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17eeeb-695f-473c-b0c2-92d8e29eba2f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-33431db1e46851bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d290cce-aa79-4b3d-b9f7-68cbf28a32c5",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-abfcc407541f339f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "### Newton's method for systems of equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb600ec-c09b-4f3a-8cba-a6018a33d96c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cae03da525c3bcaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 1:** Complete the following implementation of Newton's method for systems of equations (Algorithm 1.7 in the lecture notes).\n",
    "\n",
    "*Hint:* To solve an equation of the form $Ax = b$ for $x$, where $A \\in \\mathbb{R}^{n \\times n}$, $b \\in \\mathbb{R}^{n}$, and $x \\in \\mathbb{R}^{n}$, use `x = np.linalg.solve(A, b)`. To compute the norm $\\lVert x \\rVert$ of a vector $x$, use `np.linalg.norm(x)`.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4055d9c-5484-4177-8f6c-2a0e49068b95",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a226ef1dfd4e3222",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def newtonsys(F, JF, x0, tol, nmax):\n",
    "    x = []  # list of iterates\n",
    "    x.append(x0)\n",
    "    r = []  # list of increments\n",
    "    r.append(tol + 1)\n",
    "    k = 0  # iteration counter\n",
    "    while r[-1] > tol and k < nmax:\n",
    "        ### BEGIN SOLUTION\n",
    "        dx = np.linalg.solve(JF(x[-1]), -F(x[-1]))\n",
    "        x.append(x[-1] + dx)\n",
    "        r.append(np.linalg.norm(x[-1] - x[-2]))\n",
    "        k = k + 1\n",
    "        ### END SOLUTION\n",
    "    return x, r, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb363d16-84af-4b7c-a510-4f246d761bbe",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ffb52f9122775c6a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us consider the nonlinear system $\\mathbf{F}({\\bf x})=\\mathbf{0}$ with\n",
    "\n",
    "$$\n",
    "\\mathbf{F}({\\bf x})\n",
    "%F(x_1,x_2)\n",
    "=\\left[\n",
    "\\begin{array}{lcl}\n",
    "e^{x_1^2+x_2^2}-\\alpha \\\\\n",
    "e^{x_1^2-x_2^2}-1\n",
    "\\end{array}\n",
    "\\right],\n",
    "$$\n",
    "where the parameter $\\alpha$ takes the values $1$ or $e$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12147212-0dea-486f-a32b-94ea938553ae",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b8fdc0909564f89f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 2 (Theoretical):** Compute the Jacobian matrix $J_\\mathbf{F}$ associated to the nonlinear system and the first Newton iteration in the case $\\alpha=e$ with ${\\bf x}^{(0)}=(1,1)^\\top$ as initial point.\n",
    "</div>\n",
    "\n",
    "=== BEGIN MARK SCHEME ===\n",
    "\n",
    "We have\n",
    "$$\n",
    "J_\\mathbf{F}({\\bf x})=\n",
    "\\left[\n",
    "\\begin{array}{lc}\n",
    "2 x_1 e^{x_1^2 + x_2^2}  &   2 x_2 e^{x_1^2 + x_2^2}  \\\\\n",
    "2 x_1 e^{x_1^2 - x_2^2}  &  - 2 x_2 e^{x_1^2 - x_2^2}\n",
    "\\end{array}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "The first Newton iteration ${\\bf x}^{(1)}$ is given by ${\\bf x}^{(1)}={\\bf x}^{(0)}+\\delta{\\bf x}$, where $\\delta{\\bf x}$ is the solution of\n",
    "\n",
    "$$\n",
    "J_{\\bf F}({\\bf x}^{(0)})\\delta{\\bf x}=-\\mathbf{F}({\\bf x}^{(0)}).\n",
    "$$\n",
    "\n",
    "For ${\\bf x}^{(0)}=(1,1)^\\top$ we have\n",
    "\n",
    "$${\\bf F}({\\bf x}^{(0)})=\\left[\n",
    "\\begin{array}{c}\n",
    "e(e-1) \\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\quad \\text{and} \\quad\n",
    "J_{\\bf F}({\\bf x}^{(0)})=\\left[\n",
    "\\begin{array}{lc}\n",
    "2e^2  &  2e^2  \\\\\n",
    "2  &  - 2\n",
    "\\end{array}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "This means we have to solve the system\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{lc}\n",
    "2e^2  &  2e^2  \\\\\n",
    "2  &  - 2\n",
    "\\end{array}\n",
    "\\right]\\left[\n",
    "\\begin{array}{c}\n",
    "\\delta x_1 \\\\\n",
    "\\delta x_2\n",
    "\\end{array}\n",
    "\\right]=\\left[\n",
    "\\begin{array}{c}\n",
    "e(1-e) \\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "The second equation gives us $\\delta x_1=\\delta x_2$ and, replacing this result in the first one, we get $\\delta x_1=\\frac{1-e}{4e}$, namely $\\delta{\\bf x}=\\frac{1-e}{4e}{\\bf x}^{(0)}$. Finally, we find ${\\bf x}^{(1)}=(\\frac{1-e}{4e}+1,\\frac{1-e}{4e}+1)^\\top\\approx (0.84197,0.84197)^\\top$.\n",
    "\n",
    "=== END MARK SCHEME ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db16f64f-309a-4ae1-b219-db304678f2ee",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-196f6af8f39ef6f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Exercise 3:** Fix a tolerance of `tol = 1e-8` and a maximal number of iterations `nmax=1000`. Apply the vector Newton method for both cases $\\alpha=1$ and $\\alpha=e$, using as initial value ${\\bf x}_1^{(0)}=(1/10,1/10)^\\top$ and ${\\bf x}_e^{(0)}=(1,1)^\\top$, respectively. How do the result and the number of iterations change? What is the order of convergence of the method in each case?\n",
    "\n",
    "*Hint:* Try to work as much with NumPy arrays as possible, since you can do mathematical operations with them (`+`, `-`, ...), which are not possible between lists. To convert a Python list `l = [1, 2, 3]` to a NumPy array, use `np.array(l)`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67187a2d-37ef-4e9a-b2a7-c24da24c41f8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** Choosing the initial point can strongly influence if, and to which zero, Newton's method converges. In general, the closer to the zero the initial point is chosen, the better the method will converge to that zero. For one-dimentional functions with one input, we can often visually determine a good starting point. Once we move to two or more  inputs, we may need to use a few iterations of another method (bisection, ...) to find a suitable stating point.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c7559d-d94f-45ac-8bf8-dc08b53b6b03",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c27b83ac14a5ea4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def F(x, alpha):\n",
    "    ### BEGIN SOLUTION\n",
    "    return np.array([np.exp(x[0] ** 2 + x[1] ** 2) - alpha, np.exp(x[0] ** 2 - x[1] ** 2) - 1])\n",
    "    ### END SOLUTION\n",
    "\n",
    "def JF(x):\n",
    "    ### BEGIN SOLUTION\n",
    "    return np.array([[2 * x[0] * np.exp(x[0] ** 2 + x[1] ** 2),\n",
    "                     2 * x[1] * np.exp(x[0] ** 2 + x[1] ** 2)],\n",
    "                    [2 * x[0] * np.exp(x[0] ** 2 - x[1] ** 2),\n",
    "                     -2 * x[1] * np.exp(x[0] ** 2 - x[1] ** 2)]])\n",
    "    ### END SOLUTION\n",
    "\n",
    "# the function F when α = 1\n",
    "def F_1(x):\n",
    "    return F(x, alpha=1)\n",
    "\n",
    "# the function F when α = e\n",
    "def F_e(x):\n",
    "    return F(x, alpha=np.e)\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "x0_1 = [1/10, 1/10]\n",
    "x0_e = [1, 1]\n",
    "\n",
    "tol = 1e-8\n",
    "nmax = 1000\n",
    "\n",
    "x_1, r_1, niter_1 = newtonsys(F_1, JF, x0_1, tol, nmax)\n",
    "x_e, r_e, niter_e = newtonsys(F_e, JF, x0_e, tol, nmax)\n",
    "\n",
    "print(f\"converged to x_1 = {x_1[-1]} after {niter_1} iterations\")\n",
    "print(f\"converged to x_e = {x_e[-1]} after {niter_e} iterations\")\n",
    "\n",
    "plt.semilogy(range(niter_1), np.array(r_1[1:]) / np.array(r_1[:-1]), marker=\"o\", label=r\"$p=1$\")\n",
    "plt.semilogy(range(niter_1), np.array(r_1[1:]) / np.array(r_1[:-1]) ** 2, marker=\"o\", label=r\"$p=2$\")\n",
    "plt.ylabel(r\"increment ratio $\\|| x^{(k + 1)} - x^{(k)} \\|| / \\|| x^{(k)} - x^{(k - 1)} \\||^{p}$\")\n",
    "plt.xlabel(r\"iteration $k$\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.semilogy(range(niter_e), np.array(r_e[1:]) / np.array(r_e[:-1]), marker=\"o\", label=r\"$p=1$\")\n",
    "plt.semilogy(range(niter_e), np.array(r_e[1:]) / np.array(r_e[:-1]) ** 2, marker=\"o\", label=r\"$p=2$\")\n",
    "plt.ylabel(r\"increment ratio $\\|| x^{(k + 1)} - x^{(k)} \\|| / \\|| x^{(k)} - x^{(k - 1)} \\||^{p}$\")\n",
    "plt.xlabel(r\"iteration $k$\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# We see that the Newton method converges with second order in the case α = e\n",
    "# whereas in the case α = 1, it only converges with first order. This is due\n",
    "# to the fact that det(Jf([0, 0])) = 0.\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2de447-f193-491e-a897-01979c2a9d98",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b13248f82404de1d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "### Damped Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f804b-7718-431d-9c70-a4cce20d0352",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6b4963af79f1e42f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We provide you with the implementation of Newton's method to find the zero of a function $g$ which you've completed in last week's notebook. However, there is one slight modification: we introduce a damping factor $0 < \\gamma \\leq 1$ in the iteration\n",
    "\n",
    "\\begin{equation*}\n",
    "     x^{(k + 1)} = x^{(k)} - \\gamma \\frac{f(x^{(k)})}{f'(x^{(k)})}.\n",
    "\\end{equation*}\n",
    "\n",
    "For $\\gamma = 1$ we end up with the standard Newton's method.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** Damping is a technique that reduces the size of the Newton step to ensure more stable behavior, especially in cases where the method without damping might oscillate too much or fail to converge. Instead of taking the full Newton step, the damped method takes only a portion of the step, usually scaled by a factor $\\gamma_k$, where $0 < \\gamma_k \\leq 1$, which changes in every iteration $k$. The advantage is that it's more stable in cases where derivatives exhibit large oscillations or when the system is poorly conditioned. For simplicity, we only consider a fixed damping $\\gamma_k = \\gamma$ for all iterations $k$ in our implementation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f5957-9e82-46a9-8c7b-e65a0fe0088a",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ba225666f25a1b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def newton_damped(f, df, x0, tol, nmax, gamma):\n",
    "    x = []  # list of iterates\n",
    "    x.append(x0)\n",
    "    r = []  # list of increments\n",
    "    r.append(tol + 1)\n",
    "    k = 0  # iteration counter\n",
    "    while r[-1] > tol and k < nmax:\n",
    "        x.append(x[-1] - gamma * f(x[-1]) / df(x[-1]))\n",
    "        r.append(abs(x[-1] - x[-2]))\n",
    "        k = k + 1\n",
    "    return x, r, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7e83f6-a8f1-4751-90ef-240e47d2a2d4",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72eb17f4b1e31f3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Further, we provide you with a Python function which, given the function $f$ on which the Newton iteration $x = [x^{(0)}, x^{(1)}, \\dots, x^{(k)}]$ has been computed, visualizes the iteration in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d522f-a5d0-49ce-acf3-84c654bb9628",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1bc5eccac237e226",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_iteration(f, x):\n",
    "    x = np.asarray(x)\n",
    "    a = np.min(x) - 1\n",
    "    b = np.max(x) + 1\n",
    "    x_lin = np.linspace(a, b, 100)\n",
    "    plt.plot(x_lin, f(x_lin), label=\"$f(x)$\")\n",
    "    plt.plot(x, f(x), marker=\"o\", label=\"iteration\")\n",
    "    plt.xlabel(r\"$x$\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5940c47-8e8e-492d-b41f-902828468a53",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f234ba6ea321b52d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The plot shows the function $f$, as well as the iterates represented with the points $(x^{(k)}, f(x^{(k)})), k=0, 1, \\dots$ along the function $f$. These points are additionally connected with a line, to emphasize the order in which they were obtained by the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c2621-d9db-4830-8afe-64d47d8ae1df",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a0431dc2cc621b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Exercise 4:** For the function $f(x) = 1 - e^{-x^2}$, run the damped Newton's method from the starting point $x_0 = 1.5$ on a tolerance of $10^{-5}$ for at most $1000$ iterations. Play around with the values of the damping parameter $0 < \\gamma \\leq 1$, and observe what effect it has on the convergence. Can you find a damping $\\gamma$, for which the method converges in less iterations than the standard method ($\\gamma = 1$)?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db171e2-3210-43cf-a205-c130a2719d8c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fd470f158b486d3f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "def f(x):\n",
    "    return 1 - np.exp(- x**2)\n",
    "\n",
    "def df(x):\n",
    "    return 2 * x * np.exp(- x**2)\n",
    "\n",
    "x0 = 1.5\n",
    "tol = 1e-5\n",
    "nmax = 1000\n",
    "\n",
    "# Standard Newton's method\n",
    "gamma = 1.0\n",
    "x, r, k = newton_damped(f, df, x0, tol, nmax, gamma)\n",
    "plot_iteration(f, x)\n",
    "print(f\"converged in {k} iterations\")\n",
    "# Observation: Since the derivative at the starting point is comparatively small, the first step size is very big.\n",
    "# In fact, we jump to the other side of the zero, which could have lead to certain issues.\n",
    "\n",
    "# Very strong damping\n",
    "gamma = 0.1\n",
    "x, r, k = newton_damped(f, df, x0, tol, nmax, gamma)\n",
    "plot_iteration(f, x)\n",
    "print(f\"converged in {k} iterations\")\n",
    "# Observation: A strong damping leads to a very stable convergence. However, many more iterations are needed.\n",
    "\n",
    "# Damping for which it takes less iterations\n",
    "gamma = 0.94\n",
    "x, r, k = newton_damped(f, df, x0, tol, nmax, gamma)\n",
    "plot_iteration(f, x)\n",
    "print(f\"converged in {k} iterations\")\n",
    "# Observation: Usually, the damped Newton's method converges slower. The fact that we could find a parameter γ for \n",
    "# which the method converges faster is more of a lucky coincidence, and not usually exploited in practice.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522d4e6b-ec82-480a-9ab9-a0bab9a30b10",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-21702acd622409d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "### Simple polynomial interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc5174d-6514-4274-ae6c-36e2ce86f000",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4cb2537f66cd892a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have the following data\n",
    "\n",
    "\\begin{align*}\n",
    "x_1=0 \\quad  & y_1=12, \\\\\n",
    "x_2=1 \\quad  & y_2=18, \\\\\n",
    "x_3=2 \\quad  & y_3=6.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6687ccc7-f4f6-4595-a346-cc232f139ec0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eae9b3444545bcc0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Exercise 5:** Find the second order polynomial interpolating the data by setting up and solving the linear system with the Vandermonde matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a279338-80cc-43df-8142-ce1966a9fdc1",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9d483aa33a68eb6d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "V = np.array([[1, 0, 0],\n",
    "              [1, 1, 1],\n",
    "              [1, 2, 4]])\n",
    "\n",
    "y = np.array([12, 18, 6])\n",
    "a = np.linalg.solve(V, y)\n",
    "print(f\"the interpolating polynomial is p₂(x) = {a[0]} + {a[1]} * x + {a[2]} * x^2\")\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f122b4c7-4a5f-4238-be7d-d99b2fc31bcc",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7e65398ebdd6886a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Exercise 6:** Find the second order polynomial interpolating the data by using `np.polyfit`, evaluate the resulting polynomial at $100$ evenly spaced points in the interval $[-1, 3]$ by using `np.polyval`, and use them to visualize the polynomial on the same plot with the data.\n",
    "\n",
    "*Hint:* To plot individual data points, use `plt.scatter`. To find out more about a function's inputs and outputs, use `help(function_name)`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d88dcf-b520-4edf-ae20-680ffb1a217c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9b9bb5f23dec69e1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "x = np.array([0, 1, 2])\n",
    "a = np.polyfit(x, y, 2)\n",
    "x_lin = np.linspace(-1, 3, 100)\n",
    "y_lin = np.polyval(a, x_lin)\n",
    "plt.scatter(x, y, label=r\"data $(x_i, y_i)$\")\n",
    "plt.plot(x_lin, y_lin, label=r\"interpolant $p_2(x)$\")\n",
    "plt.grid()\n",
    "plt.ylabel(r\"$y$\")\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cc0a3a-54bc-41be-abf0-5f1276a610f0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c76fe18176498d71",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Exercise 7 (Theoretical):** Find the three polynomials of the Lagrangian basis corresponding to the points $x_1=0, x_2=1$ and $x_3=2$.\n",
    "\n",
    "*Hint:* Use the formula given in Definition 2.2 of the lecture notes.\n",
    "</div>\n",
    "\n",
    "=== BEGIN MARK SCHEME ===\n",
    "\n",
    "Using the formula from Definition 2.2\n",
    "\n",
    "\\begin{align*}\n",
    "\\phi_i(x)=\\prod_{\\stackrel{j=1}{j\\neq i}}^3 \\frac{x-x_j}{x_i-x_j}, \\quad \\quad i=1,2,3.\n",
    "\\end{align*}\n",
    "\n",
    "we find the Lagrangian polynomials by replacing the values of $x_1 = 0$, $x_2 = 1$, and $x_3 = 2$ to get\n",
    "\n",
    "\\begin{align*}\n",
    "& \\phi_1(x)=\\frac{x-x_2}{x_1-x_2}\\cdot \\frac{x-x_3}{x_1-x_3}= \\frac{(x-1)(x-2)}{2}\\\\\n",
    "& \\phi_2(x)=\\frac{x-x_1}{x_2-x_1}\\cdot \\frac{x-x_3}{x_2-x_3}= -x(x-2)\\\\\n",
    "& \\phi_3(x)=\\frac{x-x_1}{x_3-x_1}\\cdot \\frac{x-x_2}{x_3-x_2}= \\frac{x(x-1)}{2}.  \n",
    "\\end{align*}\n",
    "\n",
    "=== END MARK SCHEME ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3786c850-1996-4a99-a81e-5b05ade12fdb",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-947dc5151412a6ab",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Exercise 8 (Theoretical):** Using your result from Exercise 7, find the second order polynomial interpolating the data. Compare the resulting polynomial to what you've found in Exercise 5.\n",
    "</div>\n",
    "\n",
    "=== BEGIN MARK SCHEME ===\n",
    "\n",
    "The sought polynomial is given by\n",
    "\n",
    "$$\n",
    "p_2(x)= y_1 \\phi_1(x) + y_2 \\phi_2(x) + y_3 \\phi_3(x).\n",
    "$$\n",
    "\n",
    "Inserting the Lagrange polynomials from Exercise 7 and the $y$-data, we get\n",
    "\n",
    "\\begin{align*}\n",
    "p_2(x)=& 6(x-1)(x-2)-18 x(x-2)+3x(x-1)\\\\\n",
    "=& -9 x^2 + 15 x + 12.\n",
    "\\end{align*}\n",
    "\n",
    "This is the same polynomial as we've found in Exercise 5.\n",
    "\n",
    "=== END MARK SCHEME ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa02af-bce4-4058-98b8-27eecf831322",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-231501d536ca7f10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "### Interpolating functions at uniform and non-uniform nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c7316-e98c-4c06-9230-698768590eae",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cf07ec01b52c57c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We are given the data $(x_i,y_i)$, $i=1,\\ldots,n+1$, where the $x_i$ are $n+1$ nodes in the interval $[a,b]=[-5,5]$ and\n",
    "the values $y_i$ come from evaluation of the function\n",
    "$$\n",
    "  g(x)= \\frac{1}{1+\\exp(4x)}\n",
    "$$\n",
    "without any measurement error, namely $y_i = g(x_i)$.  We want\n",
    "to apply polynomial interpolation to the function $g$ from the data $(x_i,y_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da944583-d2ca-4c35-bc13-326ce4bae710",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3713f2f8c72dc60b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Exercise 9:** For degrees $n=6$ and $n=14$, compute the polynomial $p_n$ interpolating the data $(x_i,y_i)$, $i=1,...,n+1$ with uniformly spaced nodes\n",
    "\n",
    "\\begin{equation*}\n",
    "    x_i = a + \\frac{i - 1}{n} (b - a), ~i=1,...,n+1.\n",
    "\\end{equation*}\n",
    "\n",
    "Visualize the function and the interpolating polynomial at $100$ evenly spaced points in the interval $[-5, 5]$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cef6c7-70d1-43b0-b1eb-557df6d46705",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7ef3e42056d8af80",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "def g(x):\n",
    "    return 1 / (1 + np.exp(4 * x))\n",
    "\n",
    "x_lin = np.linspace(-5, 5, 100)\n",
    "y_lin = g(x_lin)\n",
    "\n",
    "x_6_unif = - 5 + np.arange(6 + 1) / 6 * (5 - (- 5))\n",
    "y_6_unif = g(x_6_unif)\n",
    "a_6_unif = np.polyfit(x_6_unif, y_6_unif, 6)\n",
    "y_lin_6_unif = np.polyval(a_6_unif, x_lin)\n",
    "\n",
    "x_14_unif = - 5 + np.arange(14 + 1) / 14 * (5 - (- 5))\n",
    "y_14_unif = g(x_14_unif)\n",
    "a_14_unif = np.polyfit(x_14_unif, y_14_unif, 14)\n",
    "y_lin_14_unif = np.polyval(a_14_unif, x_lin)\n",
    "\n",
    "plt.plot(x_lin, y_lin, label=r\"function $g(x)$\")\n",
    "plt.scatter(x_6_unif, y_6_unif, label=r\"$(x_i, y_i)$ for $n = 6$\")\n",
    "plt.plot(x_lin, y_lin_6_unif, label=r\"interpolant $p_6(x)$\")\n",
    "plt.scatter(x_14_unif, y_14_unif, label=r\"$(x_i, y_i)$ for $n = 14$\")\n",
    "plt.plot(x_lin, y_lin_14_unif, label=r\"interpolant $p_{14}(x)$\")\n",
    "plt.grid()\n",
    "plt.ylabel(r\"$y$\")\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9919e8-49cb-4d38-9363-616cc7fb311c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cea090203d5bd4f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Exercise 10:** Repeat Exercise 9 for Clenshaw-Curtis nodes, i.e. nodes which are given by\n",
    "\n",
    "\\begin{equation*}\n",
    "    x_i = \\frac{a + b}{2} - \\frac{b - a}{2} \\cos\\left( \\frac{\\pi(i - 1)}{n} \\right),  ~i=1,...,n+1.\n",
    "\\end{equation*}\n",
    "\n",
    "Compare the result to Exercise 10. Explain the difference using what you've learned in the lecture.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e937f9-10d8-4a34-b8c5-93f5e31c2ef7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72d8d1630a517dcb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "x_6_cc = (5 + (- 5)) / 2 + (5 - (- 5)) / 2 * np.cos(np.pi * np.arange(6 + 1) / 6) \n",
    "y_6_cc = g(x_6_cc)\n",
    "a_6_cc = np.polyfit(x_6_cc, y_6_cc, 6)\n",
    "y_lin_6_cc = np.polyval(a_6_cc, x_lin)\n",
    "\n",
    "x_14_cc = (5 + (- 5)) / 2 + (5 - (- 5)) / 2 * np.cos(np.pi * np.arange(14 + 1) / 14) \n",
    "y_14_cc = g(x_14_cc)\n",
    "a_14_cc = np.polyfit(x_14_cc, y_14_cc, 14)\n",
    "y_lin_14_cc = np.polyval(a_14_cc, x_lin)\n",
    "\n",
    "plt.plot(x_lin, y_lin, label=r\"function $g(x)$\")\n",
    "plt.scatter(x_6_cc, y_6_cc, label=r\"$(x_i, y_i)$ for $n = 6$\")\n",
    "plt.plot(x_lin, y_lin_6_cc, label=r\"interpolant $p_6(x)$\")\n",
    "plt.scatter(x_14_cc, y_14_cc, label=r\"$(x_i, y_i)$ for $n = 14$\")\n",
    "plt.plot(x_lin, y_lin_14_cc, label=r\"interpolant $p_{14}(x)$\")\n",
    "plt.grid()\n",
    "plt.ylabel(r\"$y$\")\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Difference: Much better approximation for Clenshaw-Curti's nodes.\n",
    "# Explanation: Clenshaw-Curtis nodes have a slower growing Lebesgue constant than uniformly spaced nodes.\n",
    "# Therefore, by Definition 2.4, the interpolation is much more stable.\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957cbe9-a44e-460f-8b49-338b088138fb",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0daf4c474bf8a9b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "## The end\n",
    "\n",
    "Amazing! You have reached the end of the fourth exercise notebook. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
