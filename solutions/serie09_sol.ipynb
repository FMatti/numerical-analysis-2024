{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dafbadd-3b63-46fc-b062-9d7fa1d15bfb",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-637883ba5cd4b126",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Numerical Analysis - Fall semester 2024\n",
    "\n",
    "# Serie 09 - Numerical integration and linear systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec8299d-7975-4f46-90f2-e2cfb9bbd57b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-52bef635d9234090",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Package imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d8ea5-1422-4638-8a3d-503839c60b5c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86e5e3747363cffd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bba792a-adb9-40c2-ad2f-94c5f5dff052",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-84aa00fc84169745",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "### Error indicators for linear systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ffbfd6-0cd9-45a0-b594-9fae36475a4d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8820993a4136c45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We consider the Vandermonde matrix\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "  1 & t_1 & t_1^2 & \\cdots & t_1^{n-1}\\\\\n",
    "  1 & t_2 & t_2^2 & \\cdots & t_2^{n-1} \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "  1 & t_n & t_n^2 & \\cdots & t_n^{n-1} \n",
    "\\end{pmatrix} \\in \\mathbb{R}^{n\\times n},\n",
    "$$\n",
    "\n",
    "where the points $t_1, \\ldots, t_n$ are equispaced in the interval $[0, 1]$. For a set of points `t`, the Vandermonde matrix can be generated with `A = np.vander(t, increasing=True)`\n",
    "\n",
    "We want to solve the linear system $A\\mathbf{x}=\\mathbf{b}$ for $\\mathbf{x}$ where\n",
    "$$\n",
    "\\mathbf{b} = \\begin{pmatrix}\n",
    "  1+t_1^2 \\\\\n",
    "  1+t_2^2 \\\\\n",
    "  \\vdots \\\\\n",
    "  1+t_n^2 \n",
    "\\end{pmatrix}  \\in \\mathbb{R}^{n}.\n",
    "$$\n",
    "Clearly, the exact solution is $\\mathbf{x}=(1,0,1,0\\ldots,0)^\\top \\in \\mathbb{R}^{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1b59a6-6e9c-4400-bb75-537540f66268",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c4b90ad935697c8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 1:** For $n=4$, solve the linear system numerically using the command `np.linalg.solve`. Let $\\mathbf{x_c}$ be the obtained solution. Compute the relative error $\\varepsilon=\\|\\mathbf{x_c}-\\mathbf{x}\\| / \\|\\mathbf{x}\\|$. \n",
    "\n",
    "*Hint:* The Euclidean norm of a vector can be computed with the NumPy command `np.linalg.norm`.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef106507-71b5-40ea-b6ca-cf4750a0a7f7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25b5edf479728cc5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "n = 4\n",
    "t = np.linspace(0, 1, n)\n",
    "A = np.vander(t, increasing=True)\n",
    "b = 1 + t ** 2\n",
    "\n",
    "x_c = np.linalg.solve(A, b)\n",
    "\n",
    "x_ex = np.zeros(n)\n",
    "x_ex[[0, 2]] = 1\n",
    "err = np.linalg.norm(x_c - x_ex) / np.linalg.norm(x_ex)\n",
    "\n",
    "print(r\"relative error of obtained solution: ε = {:.4e}\".format(err))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed66291-149f-4c3d-8743-e6a9f32c9080",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-192fafc71f023982",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "An upper bound of the relative error $\\varepsilon$ is given by\n",
    "$$\n",
    "\\eta = \\kappa(A)~\\text{eps}\n",
    "$$\n",
    "where $\\kappa(A)$ is the condition number of the matrix $A$\n",
    "(which can be estimated using the command `np.linalg.cond`) and $\\text{eps}$ the rounding unit of the floating point representation (which is defined in the variable `sys.float_info.epsilon`). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b923f1-6bf4-4887-b204-6e6285cf99b4",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cbe628f9be296d89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 2:** For $n=4$, compute $\\eta$ and check if this is an upper bound on the relative error $\\varepsilon$\n",
    "computed in the previous exercise.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994043f-ad5e-4020-994e-660e0201cc80",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ac1d634f6c5b20f4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "eps = sys.float_info.epsilon\n",
    "cond_A = np.linalg.cond(A)\n",
    "eta = cond_A * eps\n",
    "\n",
    "print(r\"upper bound for relative error of obtained solution: η = {:.4e}\".format(eta))\n",
    "# We see that the bound η is an upper bound on the true relative error ε.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a8637a-6b54-412f-b991-8939befc74be",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-082097f7af39bc91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 3:** For the same linear system but for matrix sizes $n=4, 6, 8, \\ldots, 20$, visualize the relative error $\\varepsilon$, the upper bound $\\eta$, and the normalized residual $r = \\|\\mathbf{b}-A\\mathbf{x_c}\\|/\\|\\mathbf{b}\\|$ in terms of $n$, in two graphs, one in logarithmic scale (`plt.loglog`) and one in semi-logarithmic scale (`plt.semilogy`). What type of growth do we see for the  error $\\varepsilon$? Is the residual $r$ a good indicator of the error $\\varepsilon$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789ab38-7626-472d-9046-b5592e5a1c7f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eabe04df715c7588",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "n_list = 2 * np.arange(2, 11)\n",
    "\n",
    "eta_list = []\n",
    "err_list = []\n",
    "res_list = []\n",
    "\n",
    "for n in n_list:\n",
    "\n",
    "    x = np.linspace(0, 1, n)\n",
    "    A = np.vander(x, increasing=\"True\")   \n",
    "    b = 1 + x ** 2\n",
    "    x_ex = np.zeros(n)\n",
    "    x_ex[[0, 2]] = 1\n",
    "    \n",
    "    cond_A = np.linalg.cond(A) \n",
    "    x_c = np.linalg.solve(A, b)\n",
    "    \n",
    "    eta_list.append(cond_A * eps)\n",
    "    res_list.append(np.linalg.norm(b - A @ x_c) / np.linalg.norm(b))\n",
    "    err_list.append(np.linalg.norm(x_c - x_ex) / np.linalg.norm(x_ex))\n",
    "\n",
    "plt.loglog(n_list, eta_list, label=r\"upper bound $\\eta$\")\n",
    "plt.loglog(n_list, err_list, label=r\"relative error $\\varepsilon$\")\n",
    "plt.loglog(n_list, res_list, color=\"black\", linestyle=\"--\", label=r\"residual $r$\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"major\", linestyle=\"-\")\n",
    "plt.grid(True, which=\"minor\", linestyle=\"--\")\n",
    "plt.xlabel(r\"matrix size $n$\")\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(n_list, eta_list, label=r\"upper bound $\\eta$\")\n",
    "plt.semilogy(n_list, err_list, label=r\"relative error $\\varepsilon$\")\n",
    "plt.semilogy(n_list, res_list, color=\"black\", linestyle=\"--\", label=r\"residual $r$\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"major\", linestyle=\"-\")\n",
    "plt.grid(True, which=\"minor\", linestyle=\"--\")\n",
    "plt.xlabel(r\"matrix size $n$\")\n",
    "plt.show()\n",
    "\n",
    "# We notice that the bound η is quite accurate. Moreover, the relative\n",
    "# error as well as the upper bound grow linearly in the graph in semi-logarithmic\n",
    "# scale, which shows that the growth of the error is exponential:\n",
    "# ε ≈ exp(αn) for some α > 0.\n",
    "\n",
    "# However, the residual is not at all a good indicator of the error as it is always\n",
    "# on the order of 10⁻¹⁵ while the true relative error becomes quite large, of\n",
    "# the order of a few percents for n = 20. This is due to the fact that the matrix\n",
    "# is ill-conditioned.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59faaad6-8283-469a-95e8-87509fcb743d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0ae4caad8c3c188",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We now consider a different matrix\n",
    "$$\n",
    "B = \\begin{pmatrix} 2 & -1 & & &  \\\\ -1 & 2 & \\ddots & &  \\\\ & \\ddots\n",
    "& \\ddots & \\ddots &  \\\\ & &\\ddots & \\ddots &  -1 \\\\ &&&-1&2 \\end{pmatrix} \\in \\mathbb{R}^{n \\times n}\n",
    "$$\n",
    "which can be generated with the command `B = np.diag(2 * np.ones(n), 0)-np.diag(np.ones(n - 1), 1) - np.diag(np.ones(n - 1), -1)`.\n",
    "\n",
    "We want to solve the linear system $B \\mathbf{y}=\\mathbf{c}$ where $\\mathbf{c}=(2,2,\\cdots,2)^\\top \\in \\mathbb{R}^{n}$. The exact solution in this case is\n",
    "$$\n",
    "\\mathbf{y} =\n",
    "\\begin{pmatrix}\n",
    "1\\cdot(n) \\\\\n",
    "2\\cdot(n-1)\\\\\n",
    "3\\cdot(n-2)\\\\\n",
    "\\vdots \\\\\n",
    "n\\cdot 1 \n",
    "\\end{pmatrix} \\in \\mathbb{R}^{n}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfec8e3-ff3b-47e0-8041-95771b9c74fc",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c568133a88fa9b70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 4:** For $n=5, 10, \\ldots, 100$, repeat the previous exercise for the matrix $B$. Comment on the result.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6bf522-9be4-4fe7-8c49-1f6584867f06",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-06fbc99ea2e15816",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "n_list = 5 * np.arange(1, 21)\n",
    "\n",
    "eta_list = []\n",
    "err_list = []\n",
    "res_list = []\n",
    "\n",
    "for n in n_list:\n",
    "\n",
    "    y = np.linspace(0, 1, n)\n",
    "    B = np.diag(2 * np.ones(n), 0) - np.diag(np.ones(n - 1), 1) - np.diag(np.ones(n - 1), -1)\n",
    "    c = 2 * np.ones(n)\n",
    "    y_ex = np.arange(1, n + 1) * np.arange(n, 0, step=-1)\n",
    "    \n",
    "    cond_B = np.linalg.cond(B) \n",
    "    y_c = np.linalg.solve(B, c)\n",
    "    \n",
    "    eta_list.append(cond_B * eps)\n",
    "    res_list.append(np.linalg.norm(c - B @ y_c) / np.linalg.norm(c))\n",
    "    err_list.append(np.linalg.norm(y_c - y_ex) / np.linalg.norm(y_ex))\n",
    "\n",
    "plt.loglog(n_list, eta_list, label=r\"upper bound $\\eta$\")\n",
    "plt.loglog(n_list, err_list, label=r\"relative error $\\varepsilon$\")\n",
    "plt.loglog(n_list, res_list, color=\"black\", linestyle=\"--\", label=r\"residual $r$\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"major\", linestyle=\"-\")\n",
    "plt.grid(True, which=\"minor\", linestyle=\"--\")\n",
    "plt.xlabel(r\"matrix size $n$\")\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(n_list, eta_list, label=r\"upper bound $\\eta$\")\n",
    "plt.semilogy(n_list, err_list, label=r\"relative error $\\varepsilon$\")\n",
    "plt.semilogy(n_list, res_list, color=\"black\", linestyle=\"--\", label=r\"residual $r$\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"major\", linestyle=\"-\")\n",
    "plt.grid(True, which=\"minor\", linestyle=\"--\")\n",
    "plt.xlabel(r\"matrix size $n$\")\n",
    "plt.show()\n",
    "\n",
    "# In this case, the error as well as the upper bound increase much more slowly. We\n",
    "# can conclude from the upper graph that the relative error increases as ε ≈ n².\n",
    "# Moreover, the normalized residual provides this time a reasonable bound on the\n",
    "# relative error, better than the bound η based on the condition number of the matrix.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff719130-d223-461b-bee0-f84f97fccbbe",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a0ebd51555d5ee4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "### Richardson's methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c11765-4eff-495d-bb71-bb898de3f701",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e7bec0601a76426c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 5:** Complete the below implementation of the Richardson's method with stopping criterion (see Algorithm 5.2 in the lecture notes).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159aee71-9481-4d86-8b4a-ec778767eff4",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1cdab4e3dd7d16d8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def richardson(A, b, P, x_0, n_max, tol):\n",
    "    x = x_0\n",
    "    res = b - A @ x\n",
    "    niter = 0\n",
    "    while np.linalg.norm(res) > tol * np.linalg.norm(b) and niter < n_max:\n",
    "        z = np.linalg.solve(P, res)\n",
    "        ### BEGIN SOLUTION\n",
    "        x = x + z\n",
    "        res = b - A @ x\n",
    "        niter = niter + 1\n",
    "        ### END SOLUTION\n",
    "    return x, res, niter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2285ade9-605c-4a02-9b05-d659c502b618",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6f33372b25653d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We now use\n",
    "$$\n",
    "A=\n",
    "\\begin{pmatrix}\n",
    "  1 & 0.5 & 0 \\\\\n",
    "  0 & 2 & 1 \\\\\n",
    "  -1 & 1 & 1\n",
    "\\end{pmatrix}, \\quad \\mathbf{b}=\n",
    "\\begin{pmatrix}\n",
    "  2 \\\\\n",
    "  5 \\\\\n",
    "  2\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d260ba0b-2ab8-4c94-a3be-91513179e617",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dfc690a668a9cfce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 6:** Implement the Jacobi method and iteratively solve the linear system $A\\mathbf{x}=\\mathbf{b}$ using the parameters $\\mathbf{x}^{(0)}=\\mathbf{0}$, $n_{\\max}=10^4$, and $tol=10^{-5}$.\n",
    "\n",
    "*Hint:* The Jacobi method is a Richardson's method for a specific choice of $P$ (see Chapter 5.2 in the lecture notes).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ec0c7-6bc4-4f1b-8b76-ccc8e998e077",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ca2775cef669156",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def jacobi(A, b, x_0, n_max, tol):\n",
    "    ### BEGIN SOLUTION\n",
    "    P = np.diag(np.diag(A))\n",
    "    x, res, niter = richardson(A, b, P, x_0, n_max, tol)\n",
    "    ### END SOLUTION\n",
    "    return x, res, niter\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "A = np.array([[1, 0.5, 0], [0, 2, 1], [-1, 1, 1]])\n",
    "b = np.array([2, 5, 2])\n",
    "x_0 = np.zeros(3)\n",
    "n_max = 1e4\n",
    "tol = 1e-5\n",
    "\n",
    "x, res, niter = jacobi(A, b, x_0, n_max, tol)\n",
    "print(r\"obtained solution: x = {}\".format(x))\n",
    "print(r\"residual of solution: res = {}\".format(res))\n",
    "print(r\"number of iterations: niter = {}\".format(niter))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd344b83-29a7-4d08-bcd5-ded120afc045",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c236e6268ad8289",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 7:** Repeat Exercise 6 using the Gauss-Seidel method.\n",
    "\n",
    "*Hint:* The NumPy function `np.tril` can be used to extract the lower triangular part of a matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a5263c-a7a4-4b1a-95ea-1f29a037e879",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d16b29ce60202a28",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gauss_seidel(A, b, x_0, n_max, tol):\n",
    "    ### BEGIN SOLUTION\n",
    "    P = np.tril(A)\n",
    "    return richardson(A, b, P, x_0, n_max, tol)\n",
    "    ### END SOLUTION\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "A = np.array([[1, 0.5, 0], [0, 2, 1], [-1, 1, 1]])\n",
    "b = np.array([2, 5, 2])\n",
    "x_0 = np.zeros(3)\n",
    "n_max = 1e4\n",
    "tol = 1e-5\n",
    "\n",
    "P_gauss_seidel = np.tril(A)\n",
    "x, res, niter = gauss_seidel(A, b, x_0, n_max, tol)\n",
    "print(r\"obtained solution: x = {}\".format(x))\n",
    "print(r\"residual of solution: res = {}\".format(res))\n",
    "print(r\"number of iterations: niter = {}\".format(niter))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7b2ad-1e3a-4875-bcbc-1fa494ca5f8e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1611b00377c077a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us now consider the new linear system $L\\mathbf{y}=\\mathbf{f}$ for the tridiagonal matrix $$\n",
    "  L=\\begin{pmatrix} 2.5 & -1 & & &  \\\\ -1 & 2.5 & -1 & &  \\\\ & \\ddots\n",
    "    & \\ddots & \\ddots &  \\\\ & &-1 & 2.5 &  -1 \\\\ &&&-1&2.5 \\end{pmatrix} \\in \\mathbb{R}^{n \\times n}, \\quad \\mathbf{f}=\n",
    "\\begin{pmatrix}\n",
    "  1.5 \\\\\n",
    "  0.5 \\\\\n",
    "  0.5 \\\\\n",
    "  \\vdots \\\\\n",
    "  0.5 \\\\\n",
    "  1.5\n",
    "\\end{pmatrix} \\in \\mathbb{R}^{n}.\n",
    "$$\n",
    "which you can generate with the command `L = np.diag(2.5 * np.ones(n), 0) - np.diag(np.ones(n - 1), 1) - np.diag(np.ones(n - 1), -1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473eebc-b4ed-40f6-9b2b-30010db254a5",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c9bda5c4eb1f25c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 8:** For $n=4$, solve the linear system with the Jacobi and Gauss-Seidel methods you've implemented in Exercises 6 and 7. Use the initial data $\\mathbf{y}^{(0)}=\\mathbf{0}$, a tolerance of $10^{-10}$, and a maximum number of iterations $10^8$. Note down the computing time as well as the number of iterations. What can we say about the convergence of the two methods?  Which one converges faster?\n",
    "*Hint:* You can measure the time (in seconds) an operation takes with\n",
    "```python\n",
    "start_time = time.time()\n",
    "# Some operation ...\n",
    "end_time = time.time()\n",
    "run_time = end_time - start_time\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f6bc1b-48b4-489c-aa77-bc371b24a0d2",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d79263a4e3edcad8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Assemble the matrix and right-hand side\n",
    "n = 4\n",
    "L = np.diag(2.5 * np.ones(n), 0) - np.diag(np.ones(n - 1), 1) - np.diag(np.ones(n - 1), -1)\n",
    "f = 0.5 * np.ones(n)\n",
    "f[[0, -1]] = 1.5\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "y_0 = np.zeros(n)\n",
    "tol = 1e-10\n",
    "n_max = 1e8\n",
    "  \n",
    "start_time = time.time()\n",
    "y, res, niter = jacobi(L, f, y_0, n_max, tol)\n",
    "end_time = time.time()\n",
    "run_time = end_time - start_time\n",
    "print(\"Jacobi:\\n\" + 7*\"-\")\n",
    "print(r\"obtained solution: x = {}\".format(y))\n",
    "print(r\"residual of solution: res = {}\".format(res))\n",
    "print(r\"number of iterations: niter = {}\".format(niter))\n",
    "print(r\"computing time: t = {}\".format(run_time))\n",
    "\n",
    "start_time = time.time()\n",
    "y, res, niter = gauss_seidel(L, f, y_0, n_max, tol)\n",
    "end_time = time.time()\n",
    "run_time = end_time - start_time\n",
    "print(\"\\nGauss-Seidel:\\n\" + 13*\"-\")\n",
    "print(r\"obtained solution: x = {}\".format(y))\n",
    "print(r\"residual of solution: res = {}\".format(res))\n",
    "print(r\"number of iterations: niter = {}\".format(niter))\n",
    "print(r\"computing time: t = {}\".format(run_time))\n",
    "\n",
    "# We notice here that both methods converge. The Jacobi method needs 53 iterations\n",
    "# and approximately while the Gauss-Seidel method takes 27 iterations. The Gauss-Seidel\n",
    "# method is approximately twice as fast, since we have ρ(B_GS) = ρ(B_J)² < ρ(B_J) for the\n",
    "# iteration matrices B_J (Jacobi method) and B_GS (Gauss-Seidel method). See Theorem 5.1\n",
    "# in the lecture notes. \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af008d19-606b-4dc4-a3f4-0d80cda43b00",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2634a67813bc0524",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 9:** Solve the linear system $L \\mathbf{y} = \\mathbf{f}$ from above for $n=4,8,12,...,200$ with initial iterate $\\mathbf{y}^{(0)}=\\mathbf{0}$, tolerance $10^{-10}$, and maximum number of iterations $10^8$. How does the number of iterations in terms of $n$ behave? Knowing that the exact solution of the system is $\\mathbf{y}=(1,...,1)^\\top$ and denoting the approximate solution as $\\mathbf{y}_c$, plot the relative error $\\|\\mathbf{y}_c-\\mathbf{y}\\|/\\|\\mathbf{y}\\|$ and the normalized residual $\\|\\mathbf{f} - L \\mathbf{y}_c\\|/\\|\\mathbf{f}\\|$ in terms of $n$ in a graph in logarithmic scale. Is the residual a good estimator of the error? How does the condition number of $L$ change in terms of $n$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527133a2-611c-4e92-9da2-e96a48009c49",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f7debbcd055e2ef7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "tol = 1e-10\n",
    "n_max = 1e8\n",
    "\n",
    "err_list_jacobi = []\n",
    "res_list_jacobi = []\n",
    "niter_list_jacobi = []\n",
    "\n",
    "err_list_gaussseidel = []\n",
    "res_list_gaussseidel = []\n",
    "niter_list_gaussseidel = []\n",
    "\n",
    "cond_list = []\n",
    "\n",
    "n_list = 4 * np.arange(1, 51)\n",
    "for n in n_list:\n",
    "\n",
    "    L = np.diag(2.5 * np.ones(n), 0) - np.diag(np.ones(n - 1), 1) - np.diag(np.ones(n - 1), -1)\n",
    "    f = 0.5 * np.ones(n)\n",
    "    f[[0, -1]] = 1.5\n",
    "    y_0 = np.zeros(n)\n",
    "    y_ex = np.ones(n)\n",
    "    \n",
    "    y_c_jacobi, _, niter_jacobi = jacobi(L, f, y_0, n_max, tol)\n",
    "    y_c_gaussseidel, _, niter_gaussseidel = gauss_seidel(L, f, y_0, n_max, tol)\n",
    "    \n",
    "    cond_list.append(np.linalg.cond(L))\n",
    "    \n",
    "    res_list_jacobi.append(np.linalg.norm(f - L @ y_c_jacobi) / np.linalg.norm(f))\n",
    "    err_list_jacobi.append(np.linalg.norm(y_c_jacobi - y_ex) / np.linalg.norm(y_ex))\n",
    "    niter_list_jacobi.append(niter_jacobi)\n",
    "                             \n",
    "    res_list_gaussseidel.append(np.linalg.norm(f - L @ y_c_gaussseidel) / np.linalg.norm(f))\n",
    "    err_list_gaussseidel.append(np.linalg.norm(y_c_gaussseidel - y_ex) / np.linalg.norm(y_ex))\n",
    "    niter_list_gaussseidel.append(niter_gaussseidel)\n",
    "\n",
    "plt.loglog(n_list, niter_list_jacobi, label=\"Jacobi\")\n",
    "plt.loglog(n_list, niter_list_gaussseidel, label=\"Gauss-Seidel\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"major\", linestyle=\"-\")\n",
    "plt.grid(True, which=\"minor\", linestyle=\"--\")\n",
    "plt.ylabel(r\"number of iterations\")\n",
    "plt.xlabel(r\"matrix size $n$\")\n",
    "plt.show()\n",
    "\n",
    "# The number of iterations increases and then stabilizes to 103 for\n",
    "# Jacobi's method and to 57 for Gauss-Seidel.\n",
    "\n",
    "plt.title(\"Jacobi method\")\n",
    "plt.loglog(n_list, err_list_jacobi, label=r\"relative error $\\varepsilon$\")\n",
    "plt.loglog(n_list, res_list_jacobi, label=r\"residual $r$\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"major\", linestyle=\"-\")\n",
    "plt.grid(True, which=\"minor\", linestyle=\"--\")\n",
    "plt.xlabel(r\"matrix size $n$\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Gauss-Seidel method\")\n",
    "plt.loglog(n_list, err_list_gaussseidel, label=r\"relative error $\\varepsilon$\")\n",
    "plt.loglog(n_list, res_list_gaussseidel, label=r\"residual $r$\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"major\", linestyle=\"-\")\n",
    "plt.grid(True, which=\"minor\", linestyle=\"--\")\n",
    "plt.xlabel(r\"matrix size $n$\")\n",
    "plt.show()\n",
    "\n",
    "plt.loglog(n_list, cond_list)\n",
    "plt.grid(True, which=\"major\", linestyle=\"-\")\n",
    "plt.grid(True, which=\"minor\", linestyle=\"--\")\n",
    "plt.ylabel(r\"condition number $\\kappa(L)$\")\n",
    "plt.xlabel(r\"matrix size $n$\")\n",
    "plt.show()\n",
    "\n",
    "# We notice that for the matrix L, the residual is a good estimator of the error.\n",
    "# We can explain this using the fact that the matrix L is well conditioned for\n",
    "# all the values of n (see condition number graph). For example, for n=200,\n",
    "# the condition number of the matrix is approximately 9. The error and the\n",
    "# residual r_k at the k-th iteration are linked by the relation\n",
    "# \n",
    "#     ||y - y_k|| / ||y|| <= κ(L) * ||r_k|| / ||f||\n",
    "#\n",
    "# Therefore, the residual is a good estimator of the error when the conditioning\n",
    "# of the matrix is close to 1.\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d07a8a-51f3-4d95-9d9b-b58fa4eaaa70",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4e07eebdd69727c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 10:** For $n=4, 8, 12, \\dots, 48$, repeat the previous exercise on the matrix\n",
    "$$\n",
    "  L=\\begin{pmatrix} 2 & -1 & & &  \\\\ -1 & 2 & -1 & &  \\\\ & \\ddots\n",
    "    & \\ddots & \\ddots &  \\\\ & &-1 & 2 &  -1 \\\\ &&&-1&2 \\end{pmatrix} \\in \\mathbb{R}^{n \\times n}\n",
    "$$\n",
    "and the right-hand side $\\mathbf{f} = (1, 0, \\dots, 0, 1)^{\\top}$, such that the exact solution will still be the same.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb15c79-6da4-4cdd-8e6c-d5c5f4ed29a8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-53da87053bc33e40",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "tol = 1e-10\n",
    "n_max = 1e8\n",
    "\n",
    "err_list_jacobi = []\n",
    "res_list_jacobi = []\n",
    "niter_list_jacobi = []\n",
    "\n",
    "err_list_gaussseidel = []\n",
    "res_list_gaussseidel = []\n",
    "niter_list_gaussseidel = []\n",
    "\n",
    "cond_list = []\n",
    "\n",
    "n_list = 4 * np.arange(1, 13)\n",
    "for n in n_list:\n",
    "\n",
    "    L = np.diag(2 * np.ones(n), 0) - np.diag(np.ones(n - 1), 1) - np.diag(np.ones(n - 1), -1)\n",
    "    f = np.zeros(n)\n",
    "    f[[0, -1]] = 1\n",
    "    y_0 = np.zeros(n)\n",
    "    y_ex = np.ones(n)\n",
    "    \n",
    "    y_c_jacobi, _, niter_jacobi = jacobi(L, f, y_0, n_max, tol)\n",
    "    y_c_gaussseidel, _, niter_gaussseidel = gauss_seidel(L, f, y_0, n_max, tol)\n",
    "    \n",
    "    cond_list.append(np.linalg.cond(L))\n",
    "    \n",
    "    res_list_jacobi.append(np.linalg.norm(f - L @ y_c_jacobi) / np.linalg.norm(f))\n",
    "    err_list_jacobi.append(np.linalg.norm(y_c_jacobi - y_ex) / np.linalg.norm(y_ex))\n",
    "    niter_list_jacobi.append(niter_jacobi)\n",
    "                             \n",
    "    res_list_gaussseidel.append(np.linalg.norm(f - L @ y_c_gaussseidel) / np.linalg.norm(f))\n",
    "    err_list_gaussseidel.append(np.linalg.norm(y_c_gaussseidel - y_ex) / np.linalg.norm(y_ex))\n",
    "    niter_list_gaussseidel.append(niter_gaussseidel)\n",
    "\n",
    "plt.loglog(n_list, niter_list_jacobi, label=\"Jacobi\")\n",
    "plt.loglog(n_list, niter_list_gaussseidel, label=\"Gauss-Seidel\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"major\", linestyle=\"-\")\n",
    "plt.grid(True, which=\"minor\", linestyle=\"--\")\n",
    "plt.ylabel(r\"number of iterations\")\n",
    "plt.xlabel(r\"matrix size $n$\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Jacobi method\")\n",
    "plt.loglog(n_list, err_list_jacobi, label=r\"relative error $\\varepsilon$\")\n",
    "plt.loglog(n_list, res_list_jacobi, label=r\"residual $r$\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"major\", linestyle=\"-\")\n",
    "plt.grid(True, which=\"minor\", linestyle=\"--\")\n",
    "plt.xlabel(r\"matrix size $n$\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Gauss-Seidel method\")\n",
    "plt.loglog(n_list, err_list_gaussseidel, label=r\"relative error $\\varepsilon$\")\n",
    "plt.loglog(n_list, res_list_gaussseidel, label=r\"residual $r$\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"major\", linestyle=\"-\")\n",
    "plt.grid(True, which=\"minor\", linestyle=\"--\")\n",
    "plt.xlabel(r\"matrix size $n$\")\n",
    "plt.show()\n",
    "\n",
    "plt.loglog(n_list, cond_list)\n",
    "plt.grid(True, which=\"major\", linestyle=\"-\")\n",
    "plt.grid(True, which=\"minor\", linestyle=\"--\")\n",
    "plt.ylabel(r\"condition number $\\kappa(A)$\")\n",
    "plt.xlabel(r\"matrix size $n$\")\n",
    "plt.show()\n",
    "\n",
    "# The necessary number of iterations increases when n increases.\n",
    "# This is due to the fact that the norm of the iteration matrices B_J and B_GS\n",
    "# gets closer and closer to 1 when n increases.\n",
    "\n",
    "# Moreover, we notice that in this case the residual is not at all a good\n",
    "# estimator of the error. This is due to the fact that the conditioning of\n",
    "# the matrix increases more and more when n increases, as we can see on the\n",
    "# fourth plot.\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac5678-9edd-43e5-8214-230acfea0087",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d5b27901d0dd7ae9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The eigenvalues of a tridiagonal matrix of the form\n",
    "\n",
    "$$\n",
    "L = \\begin{bmatrix} a & b & & &  \\\\ c & a & b & &  \\\\ & \\ddots & \\ddots & \\ddots &  \\\\ & & c & a & b \\\\ &&&c&a \\end{bmatrix} \\in\\mathbb{R}^{n\\times n}\n",
    "$$\n",
    "are $\\lambda_i(L)=a+2\\sqrt{bc}\\cos\\left(\\frac{\\pi i}{n+1}\\right)$, $i=1,...,n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943fed81-43fd-4b46-9b85-2b7db9951d9e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3d53ad275e551be4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 11 (Theoretical):** Give the condition number $\\kappa(L)$ of the tridiagonal matrix $L$ explicitly in the case where $b=c$ and $|b|\\leq a/2$, i.e. for positive semi-definite $L$. Can this explain the numerical results obtained in the two previous exercises?\n",
    "\n",
    "*Hint:* For a symmetric matrix $L$, the condition number is given by $\\kappa(L) = \\lambda_{\\max}(L) / \\lambda_{\\min}(L)$.\n",
    "</div>\n",
    "\n",
    "=== BEGIN MARK SCHEME ===\n",
    "\n",
    "The condition number of a matrix $L$, which is symmetric and positive definite, is given by\n",
    "$\\kappa(L)=\\frac{\\lambda_{\\max}(L)}{\\lambda_{\\min}(L)}$. For the considered tridiagonal\n",
    "matrix $L$, the eigenvalues are given by $\\lambda_i(L)=a+2|b|\\cos(\\pi i/(n+1))$ where $|\\cdot|$\n",
    "is the absolute value. As the maximum and the minimum of $\\cos(\\pi i/(n+1))$ are reached in\n",
    "$i=1$ and $i=n$, respectively, the maximal an minimal eigenvalues are given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_{\\max}(L)&=a+2|b|\\cos\\left(\\frac{\\pi}{n+1}\\right) \\\\\n",
    "\\lambda_{\\min}(L)&=a+2|b|\\cos\\left(\\frac{\\pi n}{n+1}\\right)=a-2|b|\\cos\\left(\\frac{\\pi}{n+1}\\right),\n",
    "\\end{align*}\n",
    "\n",
    "and the condition number of the matrix is therefore\n",
    "\n",
    "$$\n",
    "\\kappa(L)=\\frac{a+2|b|\\cos\\left(\\frac{\\pi}{n+1}\\right)}{a-2|b|\\cos\\left(\\frac{\\pi}{n+1}\\right)}.\n",
    "$$\n",
    "\n",
    "With $a=2.5$ and $b=-1$ we get for $n=200$ a condition number of $\\kappa(L)\\approx 8.995$ whereas for $a=2$ and $b=-1$, we have $\\kappa(L)\\approx 16373$ which is consistent with the numerical results obtained above.\n",
    "\n",
    "=== END MARK SCHEME ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5871b69c-bf62-46c9-b13c-0821a1f4cfa4",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2ba7c9bcdf5b69b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "### Condition number of simple matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5347b-60cb-45d0-960d-8049228103f7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e9b052897b602dcd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Given are the matrices\n",
    "\n",
    "\\begin{align*}\n",
    "  D=\n",
    "  \\begin{pmatrix}\n",
    "    d_1 & 0 \\\\\n",
    "    0   & d_2\n",
    "  \\end{pmatrix}, \\quad \n",
    "  R=\n",
    "  \\begin{pmatrix}\n",
    "    \\cos(\\theta) & -\\sin(\\theta) \\\\\n",
    "    \\sin(\\theta) & \\cos(\\theta)\n",
    "  \\end{pmatrix}, \\quad\n",
    "  L=\n",
    "  \\begin{pmatrix}\n",
    "    1 & 0 \\\\\n",
    "    a & 1\n",
    "  \\end{pmatrix},\n",
    "\\end{align*}\n",
    "\n",
    "where $d_1>d_2>0$, $0 < \\theta < 2 \\pi$ and $a=1000$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d791b068-eca2-43cb-9b23-c82494e9891b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2e3c1e89c282bb17",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 12 (Theoretical):** Compute the condition number of the matrices $D$, $R$, and $L$.\n",
    "\n",
    "*Hint:* You can also use Python to make an educated guess.\n",
    "</div>\n",
    "\n",
    "=== BEGIN MARK SCHEME ===\n",
    "\n",
    "The condition number of a matrix $A$ is defined by\n",
    "\n",
    "$$\n",
    "\\kappa(A)=\\sqrt{\\frac{\\lambda_{\\max}(A^\\top A)}{\\lambda_{\\min}(A^\\top A)}},\n",
    "$$\n",
    "\n",
    "where $\\lambda_{\\max}(A)$ is the largest eigenvalue of $A$ and $\\lambda_{\\min}(A)$ is the smallest eigenvalue of $A$. To solve\n",
    "the exercise, we have to compute the eigenvalues of the matrices $D^\\top D$, $R^\\top R$ and $L^\\top L$.\n",
    "\n",
    "**Matrix $D$**\n",
    "\n",
    "$$\n",
    "D^\\top D = \n",
    "\\begin{pmatrix}\n",
    "  d_1 & 0 \\\\\n",
    "  0   & d_2 \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "  d_1 & 0 \\\\\n",
    "  0   & d_2 \n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "  d_1^2 & 0 \\\\\n",
    "  0   & d_2^2 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and therefore $\\lambda_{\\max}(D^\\top D)=d_1^2, \\lambda_{\\min}(D^\\top D)=d_2^2$ and $\\displaystyle \\kappa(D)=\\frac{d_1}{d_2}$.\n",
    "\n",
    "**Matrix $R$**\n",
    "\n",
    "$$\n",
    "R^\\top R=\n",
    "\\begin{pmatrix}\n",
    "  \\cos(\\theta) & \\sin(\\theta) \\\\\n",
    "  -\\sin(\\theta) & \\cos(\\theta)\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "  \\cos(\\theta) & -\\sin(\\theta) \\\\\n",
    "  \\sin(\\theta) & \\cos(\\theta)\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "  1 & 0 \\\\\n",
    "  0 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and therefore $\\lambda_{\\max}(R^\\top R)=\\lambda_{\\min}(R^\\top R)=1$ and $\\displaystyle \\kappa(R)=1$.\n",
    "\n",
    "**Matrix $L$**\n",
    "\n",
    "$$\n",
    "L^\\top L=\n",
    "\\begin{pmatrix}\n",
    "  1 & a \\\\\n",
    "  0 & 1\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "  1 & 0 \\\\\n",
    "  a & 1\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "  a^2+1 & a \\\\\n",
    "  a & 1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "The eigenvalues of $L^\\top L$ solve $\\det(L^\\top L-\\lambda I)=0,$ namely\n",
    "\n",
    "$$\n",
    "0 = \\det\n",
    "\\begin{pmatrix}\n",
    "  a^2+1-\\lambda & a \\\\\n",
    "  a & 1-\\lambda\n",
    "\\end{pmatrix}\n",
    "= (a^2+1-\\lambda)(1-\\lambda) - a^2\n",
    "= \\lambda^2 -(2+a^2)\\lambda + 1,\n",
    "$$\n",
    "\n",
    "whence we get\n",
    "\n",
    "$$\n",
    "\\lambda_{\\max}(L^\\top L) = \\frac{2+a^2+a\\sqrt{a^2 + 4}}{2} \\approx 10^6, \\quad\n",
    "\\lambda_{\\min}(L^\\top L) = \\frac{2+a^2-a\\sqrt{a^2 + 4}}{2} \\approx 10^{-6}\n",
    "$$\n",
    "\n",
    "and therefore $\\kappa(L) \\approx 10^6$.\n",
    "\n",
    "=== END MARK SCHEME ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682480a2-ab7d-48a2-b1fa-517b9d5d3461",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0ffcfeef32899e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "### Convergence of Jacobi's method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00451d17-b5b0-45f6-bd7c-ea7fd72ba562",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7642445980a17a6f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We consider the linear system $A \\mathbf{x}=\\mathbf{b}$ with\n",
    "\n",
    "$$\n",
    "  A = \\begin{pmatrix}\n",
    "        1 & 0 & \\alpha \\\\\n",
    "        \\beta & 1 & 0 \\\\\n",
    "        0 & \\gamma & 1 \n",
    "       \\end{pmatrix}\n",
    "      , \\qquad \\mathbf{b}=  \\begin{pmatrix} 1\\\\ 3\\\\ 2\\end{pmatrix}\n",
    "$$\n",
    "where $\\alpha, \\beta, \\gamma \\in \\mathbb{R}$ are given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2d07b-c144-49b1-960f-c44a66a34d41",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-62e05fed462f74a2",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 13 (Theoretical):** Write Jacobi's method to solve the linear system $A\\bf{x}=\\bf{b}$. That is, express the new iterates $x_1^{(k+1)}$, $x_2^{(k+1)}$, and $x_3^{(k+1)}$ in terms of the previous iterates $x_1^{(k)}$, $x_2^{(k)}$, and $x_3^{(k)}$.\n",
    "</div>\n",
    "\n",
    "=== BEGIN MARK SCHEME ===\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}{l}\n",
    "x_1^{(k+1)}=\\frac{1}{a_{11}}\\, \\left( b_1 - a_{12}\\, x_2^{(k)} - a_{13}\\, x_3^{(k)} \\right)= 1 - \\alpha\\, x_3^{(k)} \\\\[5mm]\n",
    "x_2^{(k+1)}=\\frac{1}{a_{22}}\\, \\left( b_2 - a_{21}\\, x_1^{(k)} - a_{23}\\, x_3^{(k)} \\right)= 3 - \\beta\\, x_1^{(k)} \\\\[5mm]\n",
    "x_3^{(k+1)}=\\frac{1}{a_{33}}\\, \\left( b_3 - a_{31}\\, x_1^{(k)} - a_{32}\\, x_2^{(k)} \\right)= 2 - \\gamma\\, x_2^{(k)}.\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "=== END MARK SCHEME ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ba702-de45-48e3-998f-faa9c7648d0a",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-63470da636d4d851",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 14 (Theoretical):** Compute the vector ${\\bf{x}}^{(1)}$ obtained after the first iteration, from the initial vector ${\\bf{x}}^{(0)}=(1,1,1)^\\top$.\n",
    "</div>\n",
    "\n",
    "=== BEGIN MARK SCHEME ===\n",
    "\n",
    "We just have to choose in the previous formulas $k=0$ and\n",
    "$x_1^{(0)}=x_2^{(0)}=x_3^{(0)}=1$. We get\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}{l}\n",
    "x_1^{(1)}= 1 - \\alpha\\, x_3^{(0)} = 1 - \\alpha \\\\[5mm]\n",
    "x_2^{(1)}= 3 - \\beta\\, x_1^{(0)}=3 -\\beta \\\\[5mm]\n",
    "x_3^{(1)}= 2 - \\gamma\\, x_2^{(0)} =2-\\gamma.\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "=== END MARK SCHEME ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02962065-acef-4f31-9e62-539f40a9b2bd",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3d76369d26e6aa3e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 15 (Theoretical):** Find a condition on $\\alpha$, $\\beta$, $\\gamma$ which ensures that the Jacobi method converges.\n",
    "</div>\n",
    "\n",
    "=== BEGIN MARK SCHEME ===\n",
    "\n",
    "The iteration matrix for the Jacobi method is given by\n",
    "\n",
    "$$\n",
    "  B_J \n",
    "  = \\Big( \\operatorname{diag}(A) \\Big)^{-1}\\Big( \\operatorname{diag}(A)-A\\Big) \n",
    "  =I-\\operatorname{diag}(A)^{-1}A =\n",
    "                   \\begin{pmatrix}\n",
    "                         0 & 0 & -\\alpha \\\\\n",
    "                         -\\beta & 0 & 0 \\\\\n",
    "                         0 & -\\gamma & 0\n",
    "                        \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "The eigenvalues of $B_J$ are given by\n",
    "\n",
    "$$\n",
    "            \\det \\begin{pmatrix}\n",
    "                         \\lambda & 0 & \\alpha \\\\\n",
    "                         \\beta & \\lambda & 0 \\\\\n",
    "                         0 & \\gamma & \\lambda\n",
    "                        \\end{pmatrix} =\n",
    "            \\lambda^3 + \\alpha\\beta\\gamma = 0 \\quad \\Longrightarrow \\quad \n",
    "            \\lambda = (-\\alpha\\beta\\gamma)^{\\frac{1}{3}}.\n",
    "$$\n",
    "\n",
    "The method converges if and only if $\\rho(B_J) < 1$, so we require $|\\alpha\\beta\\gamma| < 1$.\n",
    "\n",
    "=== END MARK SCHEME ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c10f0-cdc1-40e6-a9a9-2b2e4dd536c1",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7e9a0eb9d4b10a95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "## The end\n",
    "\n",
    "Outstanding! You've reached the end of the ninth exercise notebook. A long one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
