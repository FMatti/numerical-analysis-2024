{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b82dd0bd-56e8-4317-8523-e2c81d6bc3a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3eb39eff5e86aa0c29baf3f80dd99dd4",
     "grade": false,
     "grade_id": "cell-030b470c6ac51770",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Numerical Analysis - Fall semester 2024\n",
    "\n",
    "# Serie 10 - Linear systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0103fd7b-986a-4a47-b20a-0d2ccfad6ae7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b8d38c890f14befdc3ff2dbfe519249",
     "grade": false,
     "grade_id": "cell-5b905d17284d63d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Package imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb4e17-3b1b-4061-9898-32cabe940d49",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b99ea1319a0463b6c98af4f012e8a35",
     "grade": false,
     "grade_id": "cell-51d332016328356d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2f63e9-9a4a-42b1-a315-28f10d729de9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9bce12633fab950b201d83db5b32e7d5",
     "grade": false,
     "grade_id": "cell-14c1e0214f42f407",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "### Convergence of Jacobi's method (From the exam of 21/01/2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d6d33-c7ce-47f1-9825-b906d73bcfe9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16ed77c35e79d87f16f8d38c5511407b",
     "grade": false,
     "grade_id": "cell-79c845784ea50757",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Given\n",
    "$$\n",
    "A =\n",
    "\\begin{pmatrix}\n",
    "  1  & \\beta \\\\\n",
    "  -2 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and $\\mathbf{b} \\in \\mathbb{R}^{2}$ which form the linear system $A\\mathbf{x}=\\mathbf{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db1c29-5d53-43ea-99c4-6e589870e941",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcec7b6a2d742fa5d6c3f432c7a9f7da",
     "grade": false,
     "grade_id": "cell-dd03d79b0a985dd3",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 1 (Theoretical):** For which values of $\\beta$ does Jacobi's method converge for any $\\mathbf{b}$ and any initial vector $\\mathbf{x}^{(0)}$? \n",
    "\n",
    "1. For $\\beta \\in (-1, 0]$\n",
    "\n",
    "2. For $\\beta = -1/2$\n",
    "\n",
    "3. For $\\beta \\in [0,1)$\n",
    "\n",
    "4. For $\\beta \\in (-1/2,1/2)$\n",
    "\n",
    "5. For $\\beta \\in (-1/2,\\infty)$\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db6d1d1-8733-47cd-87a1-e9be671fe8f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3743ebeb6de399f350776f8bff683c0",
     "grade": false,
     "grade_id": "cell-f398bd87fc88c149",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "### Gradient method and generalizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1afed20-5da3-404a-95cc-b942003e514c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce466520fbcb9d003a73f39cf8105aa5",
     "grade": false,
     "grade_id": "cell-f4169769f3e0965a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Exercise 2:** Complete the function `gradient_method` which implements the gradient method (Algorithm 5.3 in lecture notes) for iteratively solving the linear system $A \\mathbf{x} = \\mathbf{b}$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138356e-598a-4646-851d-921ddec39f19",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94ab1825a86d18ada3233bafda0e6507",
     "grade": false,
     "grade_id": "cell-9a493deabc3677ef",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_method(A, b, x_0, n_max, tol):\n",
    "    x = x_0\n",
    "    r = b - A @ x\n",
    "    k = 0\n",
    "    while np.linalg.norm(r) > tol * np.linalg.norm(b) and k < n_max:\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    return x, r, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9090d8f8-31ff-4571-aafa-20e2e007e19e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcfbfed30c671b6ebbe597067ab814ef",
     "grade": false,
     "grade_id": "cell-62c391c01606cb0b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We now want to compare the gradient method with the Jacobi method from last week. Therefore, we provide you with an implementation of the Jacobi method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1405e6-a254-45ee-9b40-6adfa07ff634",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be77a1b1d2a434797bf647c382cee06e",
     "grade": false,
     "grade_id": "cell-74a586f28587eaa6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def jacobi(A, b, x_0, n_max, tol):\n",
    "    P = np.diag(np.diag(A))\n",
    "    x = x_0\n",
    "    r = b - A @ x\n",
    "    k = 0\n",
    "    while np.linalg.norm(r) > tol * np.linalg.norm(b) and k < n_max:\n",
    "        z = np.linalg.solve(P, r)\n",
    "        x = x + z\n",
    "        r = b - A @ x\n",
    "        k = k + 1\n",
    "    return x, r, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef6487-7c46-41a8-8db0-87e8029587a4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1eb891b3d079e7506c3c89fe3cdd9d1",
     "grade": false,
     "grade_id": "cell-d5c5e63cef0e1806",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As a test case, we consider the symmetric positive definite Laplacian matrix $A \\in \\mathbb{R}^{m \\times m}$ you are already familiar with from two weeks ago. You can generate it with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f40a1b-a5f5-4db5-9397-46cf204711b8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24ad07f29c52b365c49fa02b2fda0c05",
     "grade": false,
     "grade_id": "cell-dbd3e282f0e9f9f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def laplacian_matrix(m):\n",
    "    n = int(np.sqrt(m))\n",
    "    d = np.ones(n ** 2)\n",
    "    mat = sp.sparse.spdiags([- d, 2 * d, - d], [-1, 0, 1], n, n)\n",
    "    I = sp.sparse.eye(n)\n",
    "    A = sp.sparse.kron(I, mat) + sp.sparse.kron(mat, I) \n",
    "    return A.toarray()\n",
    "\n",
    "A = laplacian_matrix(100)\n",
    "plt.title(r\"$100 \\times 100$ Laplacian matrix\")\n",
    "plt.spy(A)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07681d4-8e52-4215-8220-a7f05a0edecb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9eec7fb54355cbfb84f6ead376f403b1",
     "grade": false,
     "grade_id": "cell-8f79118cb2cd1479",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Exercise 3:** Solve the system $A \\mathbf{x} = \\mathbf{b}$ for the Laplacian matrix $A \\in \\mathbb{R}^{100 \\times 100}$ from Exercise 2 for number of iterations $n_{\\max} = 10, 20, 30, \\dots, 200$ with the Jacobi and the gradient method. We \"handpick\" the solution $\\mathbf{x} = (\\frac{1}{m}, \\frac{2}{m}, \\dots, \\frac{m}{m})^{\\top}$, and set $\\mathbf{b} = A \\mathbf{x}$. Use $\\mathbf{x}^{(0)} = \\boldsymbol{0}$ and take `tol = 0` such that the stopping criterion won't be satisfied. Plot the errors $\\lVert \\mathbf{x}_c - \\mathbf{x} \\rVert$ of the iterative solution $\\mathbf{x}_c$ for both methods against the number of iterations $n_{\\max}$ for a logarithmic $y$-axis (`plt.semilogy`). Explain what you observe.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f393d0-ae69-4b89-b14e-0c7dae16f09c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "add69605c519f263fdb5aac35eeacb62",
     "grade": false,
     "grade_id": "cell-d58e8c1f9b517430",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a2da83-7d27-4c9e-8df3-8b759cccb80c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7d4aba1572460bf7c525655db230c26",
     "grade": false,
     "grade_id": "cell-63dcdba40019bec4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Exercise 4:** Repeat the previous exercise to compare the gradient method with the conjugate gradient method. Explain the difference by analyzing the condition number of $A$ using `np.linalg.cond(A)`.\n",
    "\n",
    "*Hint:* You can use `x, _ = sp.sparse.linalg.cg(A, b, x_0, maxiter=n_max, rtol=tol)` to obtain the conjugate gradient solution `x` of the linear system with matrix `A` and right-hand side `b`, with starting vector `x_0`, maximum number of iterations `n_max`, and tolerance `tol`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69512ac-6225-4d1f-a2c2-89b2c26ee9a7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddf69aabd028591f317fc833ca53713d",
     "grade": false,
     "grade_id": "cell-23d8f61d4013aafd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5765c8-886d-4468-9632-93e24691c77f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67dbe5349f8ff0b6f04846a0fe91ed38",
     "grade": false,
     "grade_id": "cell-d801e88e8b693094",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "### Preconditioned conjugate gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44199fe-83ef-4a37-968c-3399931eabca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72e05ee92aec00a141b10b1e6fc0c0a9",
     "grade": false,
     "grade_id": "cell-046942c1a0b208cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The below function generates the matrix $A \\in \\mathbb{R}^{m \\times m}$, such that the diagonal entries are $a_{jj} = 0.5 + \\sqrt{j}, j=1, 2, \\dots, m$, and the first and $\\sqrt{m}-th$ sub- and superdiagonal are $-1$. This matrix has quite a high condition number, meaning, it is *ill-conditioned*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e162ba-c9aa-49ad-a5d8-27c0054a2d4c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6a5591c16aeb6a618f55936da9389c2",
     "grade": false,
     "grade_id": "cell-d72ac70f9fc29f42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ill_conditioned_matrix(m):\n",
    "    n = int(np.sqrt(m))\n",
    "    e = np.ones(n ** 2)\n",
    "    v = np.sqrt(np.arange(n ** 2))\n",
    "    A = sp.sparse.spdiags([-e, -e, 0.5*e + v, -e, -e], [-n, -1, 0, 1, n])\n",
    "    return A.toarray()\n",
    "\n",
    "A = ill_conditioned_matrix(100)\n",
    "plt.title(r\"$100 \\times 100$ ill-conditioned matrix\")\n",
    "plt.spy(A)\n",
    "plt.show()\n",
    "\n",
    "print(\"condition number: κ(A) = {:.3f}\".format(np.linalg.cond(A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0f8d64-9945-4dc3-abfa-309868ccfcad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "263189a4bdf89efe238ec3680b8c49b4",
     "grade": false,
     "grade_id": "cell-39476650e6021d29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Since the speed with which the conjugate gradient method convergences depends on the condition number $\\kappa(A)$, we can use a matrix $M$ such that the new condition number $\\tilde{\\kappa}(M A)$ is smaller, i.e. $M$ is a good approximation to the inverse of $A$. To measure how fast the conjugate gradient method converges for different choices of preconditioning $M$, we can use the following function `precond_conjugate_gradient_niter`, which takes as inputs a linear system with matrix `A`, a right-hand side `b`, and a preconditioner $M$, and uses the preconditioned conjugate gradient method with starting vector `x_0`, maximum number of iterations `n_max`, and tolerance `tol` to solve the system. The function returns the number of iterations it took to find a solution which satisfies the stopping criterion with tolerance `tol`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e34d33-220e-43fe-81c6-7bd3d574124a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79572ce891917abec83692f4862e83f1",
     "grade": false,
     "grade_id": "cell-f23cc2ea3a078130",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def precond_conjugate_gradient_niter(A, b, M, x_0, n_max, tol):\n",
    "    global niter\n",
    "    niter = 0\n",
    "    def counter_conjugate(arr):\n",
    "        global niter\n",
    "        niter = niter + 1\n",
    "    sp.sparse.linalg.cg(A, b, x_0, maxiter=n_max, rtol=tol, M=M, callback=counter_conjugate)\n",
    "    return niter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f3363-3791-47a8-949e-915616007f84",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6571422f2442b26e90d730b9d3cfe3b2",
     "grade": false,
     "grade_id": "cell-1bf09cc809140a45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We now want to study the dependence of the number of iterations it takes the algorithm to converge for different choices of preconditioning. \n",
    "\n",
    "* No preconditioning:\n",
    "$$\n",
    "M = I_{m}\n",
    "$$\n",
    "where $I_{m}$ is the $m \\times m$ identity matrix.\n",
    "\n",
    "* Jacobi preconditioning:\n",
    "$$\n",
    "M = \\operatorname{diag}(A)^{-1},\n",
    "$$\n",
    "i.e. the matrix with the diagonal of $A$, and all other entries being zero.\n",
    "\n",
    "* Tridiagonal preconditioning:\n",
    "$$\n",
    "M = \\operatorname{tridiag}(A)^{-1},\n",
    "$$\n",
    "i.e. the matrix which just contains the diagonal and both off-diagonals of $A$, and all other entries being zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15a61ff-637b-4a9a-9f24-77507dd5a818",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cedb655df9987c5d2dea46390b184ca1",
     "grade": false,
     "grade_id": "cell-31332c77ca2b2807",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Warning:** The NumPy function `np.diag` called on an $m \\times m$ matrix $A$ returns its diagonal as a length `m` vector, and not the matrix $\\operatorname{diag}(A)$ which is everywhere zero except for the diagonal entries, which coincide with the ones in $A$. However, `np.diag` called on a length $m$ vector will generate an $m \\times m$ diagonal matrix with this vector on its diagonal. Hence, you can generate $\\operatorname{diag}(A)$ by calling the `np.diag` function twice, i.e. `A_diag = np.diag(np.diag(A))`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b2850-cc9f-40fa-953d-0732c32ba4a8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f742b24ac45006def87dd02a9ebae0e",
     "grade": false,
     "grade_id": "cell-fee4a2e3d7bf23e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Exercise 5:** For the system $A \\mathbf{x} = \\boldsymbol{1}$ with $A \\in \\mathbb{R}^{100 \\times 100}$ generated with `ill_conditioned_matrix` and $\\boldsymbol{1} \\in \\mathbb{R}^{100}$ the vector of ones, use the function `precond_conjugate_gradient_niter` to determine how many iterations the preconditioned conjugate gradient method requires to satisfy the stopping criterion with tolerance `tol = 1e-16` for the different choices of the preconditioner $M$ mentioned above. Use a random normal vector $\\mathbf{x}^{(0)}$ (generated with `np.random.randn(m)`) as starting vector and choose `n_max` sufficiently large to not be reached before the stopping criterion is satisfied. Can you explain the difference?\n",
    "\n",
    "*Hint:* For a matrix $A$, you can extract the sub-diagonal with `np.diag(A, k=-1)` and super-diagonal with `np.diag(A, k=1)`. Same goes for generating a sub-diagonal and super-diagonal matrix from a vector.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5f04a-472e-4d4e-b9fa-eda335e50b22",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e25d48b8047199adc909a8b2f6e4c09",
     "grade": false,
     "grade_id": "cell-3cfc35a0429b5bf9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e205607-7382-4c4e-b254-a2359c76bd42",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa3a13f008a037eccf1e487af2724dfa",
     "grade": false,
     "grade_id": "cell-2a3a08b86027a809",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "## The end\n",
    "\n",
    "Easy! You have already finished the tenth exercise notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
